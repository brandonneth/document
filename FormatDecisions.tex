\chapter{Data Transformations}\label{chap:FormatDecisions}

\section{Introduction}

The gap between CPU speeds and memory latency recurs throughout computing history, prompting ever-more complex architectural solutions.
From the 384 byte ``B-store'' of the 1960 Ferranti Atlas~\cite{ferranti1960features} to the multi-level cache systems of today, memory hierarchy has endured as the solution.
The challenge is handed from the architect to the programmer, who must write programs that make use of this hierarchy efficiently.
There are two ways of doing this: changing the order in which data is accessed (loop schedules) and changing the order in which data is stored (data layouts).
The previous chapter focused on schedule transformations across multiple loops. 
This chapter focuses on runtime data layout transformations among canonical data layouts.

%Motivate the incorporation of data layout optimizations into PPLs
When optimizing code by hand, these transformations are onerous to implement.
The original, modularized implementation of a computation is obscured by the changes of implementing the transformation. 
When a code is meant to run on multiple systems or evolve to support new features, these optimizations can end up being more trouble than they are worth, the performance improvement offset by the increased maintenance and porting costs.

Performance portability libraries (PPLs) --- like RAJA~\cite{hornung2014RAJA}, Kokkos~\cite{edwards2014kokkos}, and YAKL~\cite{norman2022portable} --- partially address this problem.
By breaking the description of a computation into separable components, programmers can experiment with different schedules and layouts without obscuring the code's underlying operation.
However, control of the layout of data is limited to a global scope; data cannot be reorganized with different layouts throughout a computation.
Should they wish to change the data layout mid-program, they would have to use multiple arrays for the same data, ensuring that each maintain the correct contents.
The brave soul who chooses this option must still overcome yet another obstacle: selecting the right combination of formats.
Even a modest computation of two loops that use four 2D arrays has more than 200 combinations from which to choose, so trying all possible options quickly becomes infeasible. 

%Motivate the problems that arise from incorporation into PPLs
The transformations under consideration here, for example switching a 2D matrix from column- to row-major storage, are well known.
They have been incorporated as automatic transformations within distributed computing contexts as source-to-source research compilers for High Performance Fortran and Fortran D~\cite{bixby1994automatic,kennedy1995automatic,kennedy1998automatic}.
More recently, others have made efforts to incorporate user control of these transformations in domain-specific languages, such as DL~\cite{sung2012dl}, ExaSlang~\cite{kronawitter2018automatic} and Tiramisu~\cite{baghdadi2019tiramisu}.
These DSLs recognize the importance of giving the programmer control over how the program is optimized. %Significance of user indicating layout
However, both the automated and user-controlled approaches use compilers that are specialized, unstandardized, and in the case of HPF no longer existent.
For codes already written using PPLs, where using a standardized language and production-grade compiler is important, these approaches are not feasible.

%Focus on the challenges
Fortunately, performance portability libraries already contain data abstractions that can form the basis for supporting these transformations.
Still, there remain challenges.
First is enabling programmer control of per-loop data layout without requiring it.
Because necessary program analysis and modeling cannot be done at compile-time without modifying the compiler, it must be done at runtime instead.
This leads to the next challenge: the overhead of the analysis and performance modeling must be smaller than the improvements provided by the transformations.
Finally, important computations that benefit from layout changes, like correlation and covariance calculations, have iteration spaces where inner loop bounds depend on outer loop iterators, which is not supported by any of the three PPLs.  %Specific motivation of triangular iteration spaces

%Clear contributions
In this chapter, I present an approach that resolves these problems while balancing user control with automation, inspired by inspector-executor patterns often used in the realm of sparse computations~\cite{saltz1990run,saltz1991multiprocessors,Strout14IPDPS,strout2018sparse}.
I also present an implementation of this approach in the RAJA library, and sketch how it would be incorporated into other PPLs.
To provide optional automated layout selection, a runtime performance model uses data access information gathered from symbolic evaluation (see Section~\ref{sec:symEval}) and microbenchmarking results to solve an optimization problem based on previous approaches~\cite{bixby1994automatic}.
Rather than using an integer linear programming (ILP) formulation, I reduce runtime modeling time by using a nonlinear programming problem instead. 
This chapter makes the following contributions:
\begin{itemize}
\item An interface for combining user-specified layout choices with additional choices based on runtime modeling;
\item An optimization to the runtime benchmarking to increase the reuse of results;
\item An optimization to the model's optimization problem to reduce the number of variables by using a nonlinear formulation instead of a linear one; and 
\item Support for iteration spaces with parametric bounds, which includes triangular iteration spaces and are expanded in Chapter~\ref{chap:SparseRAJA} to support sparse iteration spaces. %connection of symbolic segments to sparse contribution
\end{itemize}

I begin with an overview of performance portability libraries, and RAJA in particular (Section 2).
Then, I expand the iteration space abstractions to support computations with relations between loop dimensions (Section 3).
Next, I introduce an interface for user-specified layout transformations between computations (Section 4).
This interface incorporates optional runtime modeling based on microbenchmarking to augment user choices without overriding them.
I build on existing automated approaches and make two key optimizations: one to reduce the number of model coefficients and one to reduce the number of model variables (Section 5).
Finally, I evaluate the model's accuracy and performance, as well as the entire system's performance and productivity for six benchmark kernels and one application case study (Section 6).
I then review related work (Section 7), and conclude (Section 8).


\section{Background}

Each of the three PPLs has its own programming model and interface, yet the abstractions present in each have similarities.
Most relevant in this chapter are their respective multi-dimensional data abstractions.

\subsection{PPL Data Abstractions}

All three PPLs have a core multi-dimensional data abstraction around which the rest of the libraries are developed.
In Kokkos and RAJA, the abstraction is the \verb.View..
In YAKL, the abstraction is the \verb.Array..
The key feature shared by these types is that they decouple how the data is referenced from how it is stored.

How exactly the libraries achieve this decoupling differs in both name and method.
In YAKL, \verb.Array. objects have a ``style'' template parameter that indicate if the object should be C style (row-major) or Fortran style (column-major). 
The \verb.View. object in Kokkos also uses a template parameter to specify the mapping of indices to memory.
Kokkos has \verb.LayoutRight. for row-major storage, \verb.LayoutLeft. for column-major, and \verb.LayoutStride. for layouts with arbitrary dimensional strides.
In RAJA, the \verb.View. object is templated by a layout type as well, but the layout type itself does not specify the ordering of the dimensions. 
Instead, layout objects are instantiated at runtime with dimensional orderings and associated with the View when it is initialized.

\subsection{Inspector-Executor Code}
\todo{motivate the need for an IE approach}
\todo{explain what it means}


\section{Parametric Iteration Spaces}
\todo{describe that the previous Ulrich and Rude work doesn't handle triangular loops.}

While PPLs break the description of a computation into clear and separable parts, their approaches for representing iteration spaces are limited in expressivity.
The expressivity varies by PPL\@.
Kokkos is limited to iteration spaces composed of contiguous ranges.
YAKL additionally supports noncontiguous dimensions with constant strides.
RAJA is the most expressive, due to its \verb.ListSegment. abstraction, which supports iteration space dimensions with arbitrary lists of index values.
However, while any individual dimension can contain arbitrary values, they can only be composed into a multidimensional iteration space using the cartesian product.

Computations with iteration space dimensions that change throughout the computation, such as those with triangular iteration spaces, cannot be represented within this framework. 
Examples of these computations include LU decomposition, Cholesky factorization, and statistical calculations like covariance and correlation.
These are important computations in scientific computing and PPLs should be able to represent them.

To address this limitation and represent iteration spaces with relations between the dimensions, I introduce the \verb.SymbolicSegment. type to the RAJA library, which also forms the basis of the sparse iteration space description framework in Chapter~\ref{chap:SparseRAJA}. 
Unlike the \verb.RangeSegment., which requires integer values for its start, stop, and stride, the \verb.SymbolicSegment. supports bounds expressions that combine other segments with constant values. 
This process involves two elements.
First, the \verb.SymbolicSegment. objects maintain their current value through the execution of a kernel. 
Second, rather than returning a static value, the \verb.begin(). and \verb.end(). methods calculate the start and stop values dynamically based on the expressions used in their construction. 

Valid bound expressions in the definition of a \verb.SymbolicSegment. can contain numeric constants and previously defined symbolic segments, combined with the four basic arithmetic operators. 
By overloading the arithmetic operators to construct expression trees, the symbolic segments can delay evaluating the bounds of the dimension until that level of the loop nest is reached.
Further, it can \textit{re}-evaluate inner bounds expressions as the outer iterator values change.

This approach evaluates the bounds expressions only at the start of each nesting level, akin to the init-statement of a standard for-loop.
Once the bounds expressions are evaluated, the dimension is treated as a standard contiguous range, but maintains the iterator value within the \verb.SymbolicSegment. object itself.
This allows the value to be used in the evaluation of bounds expressions for deeper nesting levels.

Because the expressions can contain division operations, evaluating them as integer expressions may lead to unexpected results. 
For example, as a lower bound, the expression \verb_1.5 - iSeg / 2_ resolves to $1$ when \verb.iSeg. has the value $0$. 
This is problematic, as $1$ is not greater than or equal to $1.5 - 0/2$.
The problem persists when expressions are rearranged to contain only integers; \verb.(3 - iSeg) / 2. suffers the same fate.
To address this issue, bounds are evaluated as floating point expressions then cast to integer values based on the bound type.
Lower bounds are cast to their ceiling, while upper bounds are cast to their floor.

Symbolic segments can readily express the triangular iteration spaces found in the computations mentioned above. 
Listing~\ref{triangularComparison} shows how a triangular iteration space written using a C++ nested for-loop is expressed using symbolic segments.

\begin{figure}
\begin{lstlisting}[caption={Comparison of C++ for-loop and RAJA \texttt{SymbolicSegment} representations of a loop nest with a triangular iteration space.},label=triangularComparison]
//C++ for-loop
for(int i = 0; i < Ni; i++) {
	for(int j = i; j < Nj; j++) { // j starts at i
		... 
	}
}

//RAJA SymbolicSegments
auto iSeg = make_symbolic_segment(0, Ni);
auto jSeg = make_symbolic_segment(iSeg, Nj);
auto segments = make_tuple(iSeg, jSeg);
\end{lstlisting}
\end{figure}

However, they cannot represent all possible polygonal iteration spaces. 
Figures~\ref{goodTriangles},~\ref{trapezoid1}, and~\ref{badShapes} illustrate the space of iteration spaces that can be represented with symbolic segments. 
The key restriction is that dimension bounds cannot contain cyclical dependences.
This means that if the $i$ dimension is used to define the bounds of the $j$ dimension, then the definition for the $i$ dimension cannot depend on the $j$ dimension. 
This requirement is enforced by the syntax of the abstraction, as a \verb.SymbolicSegment. cannot be forward-declared.

\begin{figure}
\begin{subfigure}{0.4\columnwidth}
\begin{tikzpicture}
\foreach \x in {-1,0,1,2,3,4,5}
\foreach \i in {-2,-1,0,1,2,3,4,5}
	\filldraw [gray] (\x, \i) circle (2pt);
\draw[thick,<->] (-0.5,0) -- (4.5,0) node[anchor=north west] {x};
\draw[thick,<->] (0,-0.5) -- (0,4.5) node[anchor=south east] {y};
\foreach \x in {0,1,2,3,4}
	\draw (\x cm,1pt) -- (\x cm,-1pt) node[anchor=north west] {$\x$};
\foreach \i in {0,1,2,3,4}
	\draw (1pt,\i cm) -- (-1pt,\i cm) node[anchor=south east] {$\i$};
	
\draw [black, very thick] (0,-1) -- (2,3) -- (4,-1) -- (0,-1);
\end{tikzpicture}
\caption{Graphical depiction of iteration space with dependent $x$ dimension.}\label{triangularIterationSpace1}
\end{subfigure}
\hspace{0.05\columnwidth}
\begin{subfigure}{0.55\columnwidth}
\begin{subfigure}{\columnwidth}
\begin{align}
	-1 \leq &y < 4 \\
	(1 + y) * 0.5 \leq &x \leq (7 - y) * 0.5
\end{align}
\caption{Constraint description of iteration space from~\ref{triangularIterationSpace1}.}\label{constraintDescription1}
\end{subfigure}

\vspace{20pt}

\begin{subfigure}{\columnwidth}
\begin{lstlisting}[]
auto y_seg = make_symbolic_segment(-1,4);
auto x_seg = make_symbolic_segment_inclusive(
							(y_seg + 1) * 0.5, 
							(7 - y_seg) / 2);
auto segments = make_tuple(y_seg, x_seg);
\end{lstlisting}
\caption{Description of iteration space from~\ref{triangularIterationSpace1} using symbolic segments.}\label{symseg1}
\end{subfigure}
\end{subfigure}

\vspace{10pt}

\begin{subfigure}{0.4\columnwidth}
\begin{tikzpicture}
\foreach \x in {-1,0,1,2,3,4,5}
\foreach \y in {-2,-1,0,1,2,3,4,5}
	\filldraw [gray] (\x, \y) circle (2pt);
\draw[thick,<->] (-0.5,0) -- (4.5,0) node[anchor=north west] {x};
\draw[thick,<->] (0,-0.5) -- (0,4.5) node[anchor=south east] {y};
\foreach \x in {0,1,2,3,4}
	\draw (\x cm,1pt) -- (\x cm,-1pt) node[anchor=north west] {$\x$};
\foreach \y in {0,1,2,3,4}
	\draw (1pt,\y cm) -- (-1pt,\y cm) node[anchor=south east] {$\y$};
	
\draw [black, very thick] (1,1) -- (4,0) -- (4,4) -- (1,1);
\end{tikzpicture}
\caption{Graphical depiction of iteration space with dependent $y$ dimension.}\label{triangularIterationSpace2}
\end{subfigure}
\hspace{0.05\columnwidth}
\begin{subfigure}{0.55\columnwidth}
\begin{subfigure}{\columnwidth}
\begin{align}
	1 \leq &x < 5 \\
	(4/3 - x/3) \leq &y \leq x
\end{align}
\caption{Constraint description of iteration space from~\ref{triangularIterationSpace2}.}\label{constraintDescription2}
\end{subfigure}

\vspace{20pt}

\begin{subfigure}{\columnwidth}
\begin{lstlisting}[]
auto x_seg = make_symbolic_segment(1,5);
auto y_seg = make_symbolic_segment(
							(4 - x_seg) / 3, 
							x_seg + 1);
auto segments = make_tuple(x_seg, y_seg);
\end{lstlisting}
\caption{Description of iteration space from~\ref{triangularIterationSpace2} using symbolic segments.}\label{symseg2}
\end{subfigure}
\end{subfigure}
\caption{Two examples of representable triangular iteration spaces.}\label{goodTriangles}
\end{figure}



\begin{figure}
\begin{subfigure}{0.4\columnwidth}
\begin{tikzpicture}
\foreach \x in {-1,0,1,2,3,4,5}
\foreach \y in {-2,-1,0,1,2,3,4,5}
	\filldraw [gray] (\x, \y) circle (2pt);
\draw[thick,<->] (-0.5,0) -- (4.5,0) node[anchor=north west] {x};
\draw[thick,<->] (0,-0.5) -- (0,4.5) node[anchor=south east] {y};
\foreach \x in {0,1,2,3,4}
	\draw (\x cm,1pt) -- (\x cm,-1pt) node[anchor=north west] {$\x$};
\foreach \y in {0,1,2,3,4}
	\draw (1pt,\y cm) -- (-1pt,\y cm) node[anchor=south east] {$\y$};
	
\draw [black, very thick] (0,0) -- (0,4) -- (3,3) -- (3,1) -- (0,0);
\end{tikzpicture}
\caption{Graphical depiction of quadrilateral iteration space with dependent $y$ dimension.}\label{trapezoidIterationSpace1}
\end{subfigure}
\hspace{0.05\columnwidth}
\begin{subfigure}{0.55\columnwidth}
\begin{subfigure}{\columnwidth}
\begin{align}
	0 \leq &x < 4 \\
	x / 3 \leq &y \leq 4 - (x / 3)
\end{align}
\caption{Constraint description of iteration space from~\ref{trapezoidIterationSpace1}}\label{trapezoidConstraint1}
\end{subfigure}

\vspace{20pt}

\begin{subfigure}{\columnwidth}
\begin{lstlisting}[]
auto x_seg = make_symbolic_segment(0,4);
auto y_seg = make_symbolic_segment_inclusive(
							x_seg / 3, 
							4 - (x_seg / 3));
auto segments = make_tuple(x_seg, y_seg);
\end{lstlisting}
\caption{Description of iteration space from~\ref{trapezoidIterationSpace1} using symbolic segments.}\label{trapseg1}
\end{subfigure}
\end{subfigure}
\caption{An example irregular quadrilateral iteration space that can be represented using symbolic segments.}\label{trapezoid1}
\end{figure}



\begin{figure}
\begin{subfigure}{0.5\columnwidth}
\begin{tikzpicture}
\foreach \x in {-1,0,1,2,3,4,5}
\foreach \y in {-1,0,1,2,3,4,5}
	\filldraw [gray] (\x, \y) circle (2pt);
\draw[thick,<->] (-0.5,0) -- (4.5,0) node[anchor=north west] {x};
\draw[thick,<->] (0,-0.5) -- (0,4.5) node[anchor=south east] {y};
\foreach \x in {0,1,2,3,4}
	\draw (\x cm,1pt) -- (\x cm,-1pt) node[anchor=north west] {$\x$};
\foreach \y in {0,1,2,3,4}
	\draw (1pt,\y cm) -- (-1pt,\y cm) node[anchor=south east] {$\y$};
	
\draw [black, very thick] (1,1) -- (3,4) -- (4,0) -- (1,1);
\end{tikzpicture}
\caption{Nonrepresentable triangular iteration space. $x$ and $y$ dimensions are defined circularly.}\label{triangularIterationSpace3}
\end{subfigure}
\begin{subfigure}{0.5\columnwidth}
\begin{subfigure}{\columnwidth}
\begin{align}
	&y \geq 4/3 - x/3 \\
	&y \leq (3 * x - 1) / 2 \\
	&x \leq 4 - y/4
\end{align}
\caption{Constraint description of iteration space in~\ref{triangularIterationSpace3}}\label{constraintDescription3}
\end{subfigure}



\end{subfigure}

\vspace{10pt}

\begin{subfigure}{0.5\columnwidth}
\begin{tikzpicture}
\foreach \x in {-1,0,1,2,3,4,5}
\foreach \y in {-1,0,1,2,3,4,5}
	\filldraw [gray] (\x, \y) circle (2pt);
\draw[thick,<->] (-0.5,0) -- (4.5,0) node[anchor=north west] {x};
\draw[thick,<->] (0,-0.5) -- (0,4.5) node[anchor=south east] {y};
\foreach \x in {0,1,2,3,4}
	\draw (\x cm,1pt) -- (\x cm,-1pt) node[anchor=north west] {$\x$};
\foreach \y in {0,1,2,3,4}
	\draw (1pt,\y cm) -- (-1pt,\y cm) node[anchor=south east] {$\y$};
	
\draw [black, very thick] (0,1) -- (1,4) -- (4,3) -- (4,1) -- (0,1);
\end{tikzpicture}
\caption{Nonrepresentable quadrilateral iteration space. $x$ and $y$ dimensions are defined circularly.}\label{trapezoidIterationSpace2}
\end{subfigure}
\begin{subfigure}{0.5\columnwidth}
\begin{align}
	-1/3 + y/3 \leq &x \leq 4 \\
	1 \leq &y \leq (13-x) / 3
\end{align}
\caption{Constraint description of iteration space in~\ref{trapezoidIterationSpace2}}\label{trapezoidConstraint2}
\end{subfigure}

\caption{Two examples of irregular iteration spaces that cannot be represented using symbolic segments. They are not representable because the dimension bounds are defined circularly.}\label{badShapes}
\end{figure}

\section{Transforming Data Layouts}

\begin{figure}
\begin{lstlisting}[caption={Changing data layouts for three Views in the \textsc{3mm} benchmark using \FormatDecisions.},
	label={FormatDecisions3MM}]
auto knl1 = make_kernel<KPOL>(segs1, [=](auto i0, auto i1, auto i2) {
	E(i0, i1) += A(i0, i2) * B(i2, i1);
});
auto knl2 = make_kernel<KPOL>(segs2, [=](auto i0, auto i1, auto i2) {
	F(i0, i1) += C(i0, i2) * D(i2, i1);
});
auto knl3 = make_kernel<KPOL>(segs3, [=](auto i0, auto i1, auto i2) {
	G(i0, i1) += E(i0, i2) * F(i2, i1);
});

auto decisions = format_decisions(ref_tuple(B,D,F), knl1, knl2, knl3);

decisions.set_format_for(B, {{1,0}}, knl1); // column-major
decisions.set_format_for(D, {{1,0}}, knl2); // column-major
decisions.set_format_for(F, {{0,1}}, knl2); // row-major
decisions.set_format_for(F, {{1,0}}, knl3); // column-major

// Generate and run the computations with format conversions
auto computation = decisions.generate();
computation();
\end{lstlisting}
\end{figure}

RAJA's existing support for changing data layouts is limited to the point of instantiation.
This work removes that barrier by providing a declarative data optimization system for specifying layout transformations between loops.
By combining user-guided layout specifications with optional automated support, \FormatDecisions{} gives the programmer more control over the optimization of their program.

\subsection{User-Guided Layout Transformations}

Consider again the computation in Listing~\ref{3MMStart}.
Throughout the computation, the data is accessed in different orders.
For example, note the Views \verb.A., \verb.B., and \verb.D..
The order in which \verb.A. is accessed is different from \verb.D. because the argument order in their accesses are different.
In contrast, while \verb.B. and \verb.D. have the same argument order, their access orders are still different because they have different layouts.
Looking at the two references to \verb.F. in kernels two and three, I can see that even access order to the same data can change through a computation.
Because different formats are optimal for different kernels, this creates an opportunity for optimization. 

My approach, shown in action in Listing~\ref{FormatDecisions3MM}, introduces a single user-facing class, appropriately named \verb.FormatDecisions..
Rather than inserting code blocks to change a View's data layout between kernel executions, the user register choices for what format the data should have during different computations. 
Once the user has finished registering their choices, they can launch the supplementary model to identify additional optimizations or immediately generate the loop chain containing the computations and the conversions.

The user registers format decisions with two methods.
The first, \verb.set_format_for()., takes as arguments the View, the desired dimensional ordering, and the kernel or kernels for which the format should be used.
The second method, \verb.set_output_format()., takes a View and a dimensional ordering and ensures the View has that layout after the sequence of computations is done executing.
Lines 13 through 17 of Listing~\ref{FormatDecisions3MM} show four such registered choices.

While \FormatDecisions{} provide a supplementary decision model that can identify additional worthwhile layout changes, its use is not mandatory.
To ``fill-in'' the user's choices with those of the model, the programmer can use the \verb.model(). method.
Because this method uses the symbolic evaluation capabilities provided by RAJALC, it can only be used when the operations are supported for symbolic evaluation.
This means that the kernels cannot make indirect accesses (\verb.a(b(i)).), or call functions within the lambdas that use the iterators.

Regardless of whether the user has used the model or not, 
the complete computation with interspersed format conversions is generated using the \verb.generate(). method.

\subsection{Selecting Decision Semantics}

At first glance, it may seem that a single method for registering format choices would be sufficient. 
Simply provide the View, the computation, and the format to use during that computation.
However, it is often desirable to specify the format the data should be in at the end of the sequence of computations, especially when the sequence is run many times, for example as part of the time step in a simulation code.
A single registering function cannot provide this capability. 

Another important consideration is the semantics of the registered decisions. 
Should the user be registering format conversions or should they be registering the format with the assumption that any necessary conversion is made by the library?
I argue that it should be the latter.
First, registering formats is more declarative, as it does not dictate exactly how or when the data is converted to the desired format.
Second, registering conversions requires specification of both the input and output formats. 
Any conversion can be specified with two format registrations, but no number of conversion registrations can specify the same thing as a single format registration.

With these considerations in mind, I decided on two methods: \verb.set_format_for. and \verb.set_output_format..
When choosing this pair, the shorter \verb.set_format. was considered, but lacks the mnemonic match between the words of the method name and the parameter order. You \textbf{set} the View to \textbf{format} X \textbf{for} computation Y. 


\section{Automatic Layout Selection}

\FormatDecisions{} supports optional automated layout selection through the \verb.model(). method.
Broadly, the system constructs a search space of possible choices, estimates their costs based on a performance model, and selects the one with the lowest.
The problem of automatic layout selection has been formulated as an integer linear programming (ILP) problem by Bixby, Kennedy, and Kremer~\cite{bixby1994automatic}.
The system used within \FormatDecisions{} is based on their formulation, but makes several modifications.
First, instead of using a static cost model, \FormatDecisions{} estimates costs using runtime microbenchmarking, which more accurately captures the performance characteristics of modern architectures.
Second, it uses a modified heuristic for determining the candidate layouts to include in the model.
Finally, \FormatDecisions{} uses a nonlinear integer programming formulation to reduce the number of decision variables, improving solve time.

This section begins with an overview of the layout selection problem and the prior formulation of Bixby, Kennedy, and Kremer.
It then details the modifications made in \FormatDecisions{}.
It concludes with six experiments that illustrate the model in action.

\subsection{The Layout Selection Problem}

Layout selection is a decision problem.
It asks: Given an array and a series of $N$ loop nests, what data layout should the array have during each loop nest to maximize performance?
Valid answers come as a list of $N+1$ layouts, one to use for each loop nest and one for the array to be in at the end.
For an isolated loop nest, the optimal answer is usually evident from a visual inspection of the code; the best layout is the one that matches the layout to the access order.
However, when considering even two loop nests in sequence, the optimal choice is no longer statically evident.
This is because the benefit of using the optimal layout for a loop nest must be weighed against the cost of converting the data from one layout to another.
The difficulty of this comparison lends itself to automation.

Automating the layout selection problem, like many other decision problems, follows a two-step process: generate a set of valid solutions, then search the set for the best one.
Different problems operationalize these two steps in different ways, but a common approach is to model the decision problem as an integer linear programming (ILP) problem.
An ILP problem consists of a set of integer variables representing decision components, linear constraints over those variables representing the relationships between parts of the decision, and a linear objective function representing the costs of making different choices.
The solution to an ILP problem is a mapping of variables to values that satisfies the constraints and minimizes the objective function.
Once the solution to the ILP model has been identified, it can be mapped back to an optimal answer to the original decision problem.

Bixby, Kennedy, and Kremer (BKK) develop such a formulation for the layout selection problem~\cite{bixby1994automatic,kennedy1995automatic,kennedy1998automatic}.
They use a restricted type of ILP problem, known as a 0--1 ILP problem, that only uses binary variables.
Because the variables can only have the values 0 or 1, they can be conceptualized as on-off ``switches.''
Their formulation uses two types of variables: layout switches and remapping switches.
For each loop nest, or ``phase'', there is one layout switch for each possible layout.
Because an array has exactly one layout at a time, one and only one switch can be on for each phase.
Remapping switches represent the layout transformations that occur between phases of the program.
At each remapping point, there is one switch for each possible layout transformation.
As with layouts, only one remapping can occur at a time, so there will be one remapping switch turned on for each remapping point.
Remapping switches are also constrained to match the layout switches.
For example, if the row-major switch is turned on for both the first and second phase, the row-to-column-major remapping switch cannot be turned on.

Using this ILP model, valid solutions to the layout selection problem are represented by constraint-satisfying configurations of the switches.
For a $D$-dimensional array over $N$ loop nests, there are $D!*(N+1)$ such valid solutions, some better than others.
The objective function makes it possible to compare the quality of different solutions, ultimately leading to the best possible choice.
In the BKK formulation, the objective function is a sum of products, with one term for each switch.
Each layout variable is multiplied by a coefficient representing the estimated cost of using that layout for that phase.
Similarly, each remapping variable is multiplied by a coefficient representing the estimated cost of performing that layout transformation.
These cost estimates only contribute to the objective function value when their associated switches are turned on.
The configuration of switches that minimizes the objective function corresponds to a solution to the layout selection problem that should provide the best performance.

The accuracy of a solution relies on the accuracy of the model used to select it.
For the layout selection problem, the most important factor is the accuracy of the cost coefficients.
In the BKK formulation, the cost coefficients are generated using a static performance model.
Thus, the quality of their layout selection system is entirely determined by the quality of this performance model.
Because the BKK performance model is static, each new machine requires a new and unique performance model be added to the system.
This limits the system's portability.




The model \FormatDecisions{} uses to solve the layout selection problem is a modified version of the BKK formulation.
There are three key differences.
First, the model is solved at runtime, rather than as part of a programming tool, using microbenchmarking results to inform the cost coefficients.
Second, the model uses an alternative approach to selecting candidate data layouts.
Third, the model uses a nonlinear objective function.



\subsection{Runtime Solving and Microbenchmarking}

Where the BKK model is incorporated into a static, pre-compilation programming tool, \FormatDecisions{} solves the layout selection problem at runtime.
Because the model is solved at runtime, \FormatDecisions{} gains two sources of information not present in the BKK formulation.
First, \FormatDecisions{} has access to program parameters, like loop bounds and array sizes that are not available to the static BKK system.
Second, \FormatDecisions{} is able to more accurately estimate the costs of different data layouts by incorporating microbenchmarking results into the objective function.
These microbenchmarks are run as part of the program's execution, so they will be tuned to the architecture, system, and even the specific node on which the program runs.
While runtime solving can incorporate previously unavailable information, it comes with runtime overhead.
This runtime overhead must be outweighed by the performance improvement of the layout optimization, so the smaller the model, the better.
The second and third modifications are motivated by this need for a quick model, but first I turn to the runtime microbenchmarking.

\todo{write about microbenchmarking}

\FormatDecisions{} uses runtime microbenchmarking to estimate the costs of using and converting among the candidate layouts. 
The results of the microbenchmarking are incorporated into the model through the coefficients on the decision variables in the objective function.
The approach considers two types of costs: use costs and conversion costs.
Use costs represent the anticipated execution time associated with accesses to the data during the computation itself.
Conversion costs represent the anticipated execution time associated with converting from one format to another in between phases.
They use different approaches to determine their contributions to the objective function.

Use cost determination is access-based and proceeds through each kernel (phase) in the loop chain.
First, the kernel is symbolically evaluated and accesses to the View being modeled are isolated.
For each access, one term will be added for each candidate layout, estimating the cost of making that access with the candidate layout.
This estimate is the product of the microbenchmarking time and the number of loop iterations that make the access.
The microbenchmark is selected to match the traversal order of the access using that layout.

Collapsing the access, schedule, and candidate layout into the traversal order allows microbenchmark results to be reused for different variables. 
For example, for a constant schedule, the access \verb.A(i,j). in row-major layout has the same traversal order as the access \verb.A(j,i). in column-major layout.
Thus, their performance can be modeled with the same microbenchmark result. 
By reusing a single timing result for multiple decision variables, the overall time necessary to run the decision model is reduced.

The microbenchmarking results from the use cost estimation can be reused further to estimate conversions costs.
The key insight here is that a conversion from one format to another is itself a computation and can be modeled as such.
The conversion computation makes one access to data in the source layout, and one access to data in the destination layout. 
Thus, the conversion cost is the sum of these two accesses.


\begin{lstlisting}[caption={Algorithm for generating the objective function's use cost terms.}]
SumExpression estimate_use_costs(knls, modeledView, candidateLayouts) {
	SumExpression useCosts();

	for knl in knls:
	  for access in knl.evaluate_symbolically():
		  if access.view == modeledView:
			  for layout in candidateLayouts:
				  traversalOrder = calc_traversal_order(knl.schedule, access.argOrder, layout);
					accessTime = time_microbenchmark(traversalOrder)
					costCoefficient = accessTime * knl.numIterations;

					decisionVar = LayoutVariable(knl, layout);
					costTerm = MultExpression(costCoefficient, decisionVar);
					useCosts.add(costTerm);
	
	return useCosts;
}
\end{lstlisting}

\begin{lstlisting}[caption={Algorithm for generating the objective function's conversion cost terms.}]
SumExpression estimateConversionCosts(knls, modeledView, candidateLayouts) {
	SumExpression conversionCosts();

	for inputLayout in candidateLayouts:
		for outputLayout in candidateLayouts:
			readTime = timeMicrobenchmark(defaultSchedule, defaultArgOrder, inputLayout);
			writeTime= timeMicrobenchmark(defaultSchedule, defaultArgOrder, outputLayout);
			costCoefficient = (readTime + writeTime) * modeledView.size();

			for inPhase in 0 to numKnls:
				outPhase = inPhase + 1;
				inVar = LayoutVariable(inPhase, inputLayout);
				outVar = LayoutVariable(outPhase, outputLayout);
				costTerm = MultExpression(inVar, outVar, costCoefficient);
				conversionCosts.add(costTerm);
	
	return conversionCosts;
}
\end{lstlisting}

\begin{lstlisting}[caption={Algorithm for microbenchmark timing. Implementation is shown for two-dimensional case. Actual implementation uses template metaprogramming to generalize the dimensionality.}]
SumExpression timeMicrobenchmark(scheduleOrder, argumentOrder, layoutOrder) {
	static std::map<...> cache; // reuse results from previous microbenchmarking

  normalizedLayoutOrder = normalize(scheduleOrder, argumentOrder, layoutOrder);

	if (cache[normalizedLayoutOrder] != 0) {
		return cache[normalizedLayoutOrder];
	}

	idx_t n = pow(modelProblemSize, 1/2.0);

	//allocate memory and create Views
	double* data1 = new double[modelProblemSize];
	double* data2 = new double[modelProblemSize];
	View2D arr1(data1, normalizedLayoutOrder);
	View2D arr2(data2, normalizedLayoutOrder);

	//fill Views
	for(int i = 0; i < modelProblemSize; i++) {
		data1[i] = std::rand();
		data2[i] = std::rand();
	}

	using Policy = NestingPolicy<2>;

	auto lam = [=](auto i, auto j) {
	  arr2(i,j) = arr1(i,j);
	};
	auto segments = make_tuple(RangeSegment(0,n), RangeSegment(0,n));

	auto knl = make_kernel<Policy>(lam, segments);

	startTimer();
	knl();
	elapsed = stopTimer();

	cache[normalizedLayoutOrder] = elapsed;
	return cache[normalizedLayoutOrder];
}
\end{lstlisting}

\todo{normalizing access order listing}

Because the cost of conversion does not change based on the number of accesses to the data, these terms can be generated without reference to their surrounding computations.
Furthermore, they can be constructed while reusing the microbenchmarking results from the use cost estimation.
This is because a conversion from one layout to another can naturally be viewed as a kernel with two accesses: one access to data in the source layout and one to data in the destination layout.
Thus, the cost coefficient becomes the size of the array multiplied by the sum of the two access estimates.


\subsection{Candidate Layout Selection}

The second modification refines how the model determines which layouts to include as candidates for selection.
The BKK formulation does not detail how their model makes this choice, but only discusses two-dimensional data, where there are only two possible orderings.
For higher-dimensional data, a simple option is to fully enumerate the possible layouts.
For two dimensional data, this gives two options; for three dimensional, six; for four, 24.
As the dimensionality increases, the number of layouts increases dramatically, and with it the modeling time.
Thus, an alternative formulation for selecting candidate layouts is needed.

Rather than using all $D!$ possible layouts, \FormatDecisions{} generates its set of candidate layouts using a seed set and a generating function.
The seed set is a small set of layouts which is expanded to a larger, final set of candidates using the generating function.
The seed set includes the original layout of the data, any layouts the user provides as part of registered decisions, and any layouts that \FormatDecisions{} calculates to match the access orders within the loop nests.
The generating function takes this set of seed layouts and generates the final set of candidates.
The generating function is user-configurable, but the default function is repeated rotation.
Rotating a layout means shifting each dimension to the right and moving the rightmost index to the leftmost position.
For example, repeatedly rotating the layout $(0,1,2)$ produces the layout $(2,0,1)$, then $(1,2,0)$, and finally $(0,1,2)$. 
Using this scheme, the number of candidate layouts is reduced from $O(D!)$ to $O(D)$, further reducing modeling time.

Repeated rotation was selected as the generating function for a confluence of reasons.
First, a good assistant surfaces options that are not always initially considered, so the generating function should produce candidate layouts that are not present in the seed set.
This excludes the identity transformation.
Second, different architectures perform better with different stride lengths.
While a CPU will do well with low-stride accesses, a GPU will perform better with large-stride accesses.
Because repeated rotation generates options where every dimension shows up once in each position, it is guaranteed to produce a wide range of stride lengths.
Finally, rotation is a staple, comprehensible transformation with transparent behavior.
This makes for easier reasoning about the model's decisions.
While repeated rotation is the default generating transformation, the user can supply an arbitrary generating function as long as it maps containers of layout permutations to containers of layout permutations.

\subsection{Nonlinear Objective Function}

Minimizing solve time also led to the third key difference between the BKK formulation and the \FormatDecisions{} formulation: the use of a nonlinear objective function.
Execution time attributable to the ILP model comes from two sources: problem initialization (setup), and objective function minimization (solve).
The setup phase initializes the objects for the space of constrained decision variables and the objective function.
The more decision variables and constraints, the longer the setup phase.
The solve phase applies the objective function to the decision space and identifies the minimum point.
This too scales with the number of decision variables and constraints.
Using a nonlinear objective function can eliminate the majority of the decision variables in the problem.
While search time per decision variable increases with this scheme, the overall model time decreases significantly.

The nonlinear formulation reduces modeling time by completely eliminating remapping variables.
Recall that the BKK formulation uses two types of decision variables in its ILP model: layout variables and remapping variables.
Further, recall that the model constraints ensure the remapping variables between two phases are matched to the layout variables for those phases.
Given these constraints, the remapping variables are completely determined by the layout variables; 
if we know the values of the layout variables, we can always reconstruct the values of the remapping variables.
So then what is the purpose of the remapping variables? 
Simply, it is to incorporate the \textit{cost of remapping} into the linear objective function.
Because there are remapping variables for each pair of layouts, a model with $O(N)$ layout variables will need $O(N^2)$ remapping variables.
As a result, these faux variables end up dominating the entire model and bloat execution time.

Remapping variables are eliminated using a replacement scheme. 
Because their values are fully determined by the layout variables, remapping variables in the decision space and constraints containing them can be removed without incident.
This only leaves the terms containing remapping variables in the objective function.
These are replace with equivalent terms that encode the removed constraints.
A remapping variable can only be switched on when the layout variables corresponding to its input and output are switched on.
This is a familiar operator: logical and––or in the realm of boolean algebra, multiplication.
Symbolically, $remap_{rowmjr \rightarrow colmjr}^{phase1 \rightarrow phase2} = layout_{rowmjr}^{phase1} * layout_{colmjr}^{phase2}$.
Thus, a replacement scheme can replace remapping variables in the objective function with products of layout variable.
This produces a nonlinear function, as it multiplies variables by other variables, but reduces the number of decision variables in the problem by a square factor.


\subsection{An Example Selection Problem}

An demonstration of the layout selection problem and the approach to solve it is instructive.
Consider the loop chain in Listing~\ref{SelectionExampleCode}, which calculates a series of matrix multiplications.
The layout selection problem in this subsection will target the \verb.A. View.
We construct the constrained decision space, then use microbenchmarking to construct the objective function.

The decision space construction starts with the selection of the candidate layout $L$.
For two-dimensional problems, the two candidates are row-major and column-major.
Within the system, these are represented as dimensional orderings--$(0,1)$ and $(1,0)$--but for clarity's sake, we use their names here: $L=\{row,col\}$.

Once the candidate layouts have been identified, we can create the layout decision variables.
For each phase of the computation, there is one variable for each layout, plus to represent the input and output layouts.
Thus, the full list of layout variables is: $[k_{row}^{0}, k_{col}^{0}, k_{row}^{1}, k_{col}^{1},k_{row}^{2}, k_{col}^{2},k_{row}^{3}, k_{col}^{3},k_{row}^{4}, k_{col}^{4}]$.
The subscript indicates the layout while the superscript indicates the phase. 
The 0 phase represents the input layout, while the $N+1$ phase represents the output layout.

The last step in creating the constrained decision space is the constraint expression. 
First, variables are constrained to be either zero or one. 
Thus, for each decision variable $k$, we add the constraint $0 \leq k \leq 1$.
Next, for each phase, exactly one layout will be selected. 
Thus, for each phase $i$, we add the constraint $\sum_{\ell \in L} k_\ell^{i} \ \ = 1$.
Optionally, a constraint can be added to match the input and output format.
In this circumstance, for each candidate layout $\ell$, we add $k_\ell^{0} = k_\ell^{N+1}$.
Finally, additional constraints are added based on decisions registered by the user, setting the appropriate variable to 1.
Figure~\ref{SelectionExampleConstrainedSpace} shows the full description of the constrained space for the example.


Constructing the objective function begins with estimating the cost of accesses to the modeled View.
Starting with the first kernel, symbolic evaluation identifies one access, \verb.A(i,k)..
Modeling this access will contribute terms for the decision variables $k_{row}^1$ and $k_{col}^1$.


\todo{figure that shows this process graphically}
\begin{figure}
\begin{subfigure}{0.4\columnwidth}
\begin{lstlisting}[caption={Example loop chain. The selection problem targets the \texttt{A} array. Code is shown using standard C++ for loops rather than RAJA kernels for the reader's familiarity.},label=SelectionExampleCode]
//knl1
for (i = 0; i < N; i++) {
	for(j = 0; j < N; j++) {
		for(k = 0; k < N; k++) {
			T1(i,j) += A(i,k) * B(k,j);
		}
	}
}
//knl2
for (i = 0; i < N; i++) {
	for(j = 0; j < N; j++) {
		for(k = 0; k < N; k++) {
			T2(i,j) += T1(i,k) * A(k,j);
		}
	}
}
//knl3
for (i = 0; i < N; i++) {
	for(j = 0; j < N; j++) {
		for(k = 0; k < N; k++) {
			T3(i,j) += A(k,i) * T2(k,j);
		}
	}
}

\end{lstlisting}
\end{subfigure}

\begin{subfigure}{\columnwidth}
Data Dimensionality: $D$

Candidate Layouts: $L$

Phases: $P$

Layout Variables: $k_{layout}^{phase}$.

Remapping Variables: $r_{inLayout \rightarrow outLayout}^{inPhase \rightarrow outPhase}$. 

One-Format Constraints: For each phase $p$, $\sum_{\ell \in L} k_{\ell}^{p} \ \ = 1$.

Binary Variable Constraints: For each phase $p$ and layout $\ell$, $0 \leq k_{\ell}^{p} \leq 1$.
\caption{ILP model parameters, variable scheme, and constraint schemes.}
\end{subfigure}

\begin{subfigure}{\columnwidth}

$\{[k_{row}^{0}, k_{col}^{0}, k_{row}^{1}, k_{col}^{1},k_{row}^{2}, k_{col}^{2},k_{row}^{3}, k_{col}^{3},k_{row}^{4}, k_{col}^{4}] \vert 
k_{row}^0 + k_{col}^0 = 1 \land 
k_{row}^1 + k_{col}^1 = 1 \land 
k_{row}^2 + k_{col}^2 = 1 \land 
k_{row}^3 + k_{col}^3 = 1 \land 
k_{row}^4 + k_{col}^4 = 1 \land 
0 \leq k_{row}^{0} \leq 1 \land
0 \leq k_{col}^{0} \leq 1 \land
0 \leq k_{row}^{1} \leq 1 \land
0 \leq k_{col}^{1} \leq 1 \land
0 \leq k_{row}^{2} \leq 1 \land
0 \leq k_{col}^{2} \leq 1 \land
0 \leq k_{row}^{3} \leq 1 \land
0 \leq k_{col}^{3} \leq 1 \land
0 \leq k_{row}^{4} \leq 1 \land
0 \leq k_{col}^{4} \leq 1 \}
$
\caption{Description of constrained decision space for example loop chain.}\label{SelectionExampleConstrainedSpace}
\end{subfigure}

\caption{Graphical representation of the layout selection problem as solved by the \FormatDecisions{} system.}
\end{figure}
 the View \verb.A. in Listing~\ref{SelectionExampleCode} 

\subsection{Six Parameters, Six Experiments}

To illustrate the workings of the layout selection model, I present its use for six synthetic programs that each vary one parameter: loop count, loop depth, memory footprint, access count, user constraint count, and data dimensionality.
Results are reported for two versions of the model.
The ``Linear Model'' panels use the BKK formulation of the optimization problem.
This means that it uses remapping variables, a linear objective function, and exhaustively enumerates the candidate layouts.
The ``Nonlinear Model'' panels use the optimized \FormatDecisions{} formulation.
This means that it does not use remapping variables, uses a nonlinear objective function, and uses the rotation heuristic for identifying candidate layouts.
For each experiment, I break the modeling time into the time spent modeling different access patterns, the time spent initializing the decision space and objective function, and the time spent identifying the optimal point within the decision space. 

\begin{figure}
\includegraphics[width=0.5\columnwidth]{LoopCount.pdf}
\caption{Changes in modeling time for different loop chain lengths.}\label{LoopCount}
\end{figure}
The first experiment, shown in Figure~\ref{LoopCount}, shows the modeling time for loop chains with between one and eight loop nests.
For this experiment, all loop nests are three-dimensional and make three accesses to two-dimensional arrays of $100\times100$ doubles. 
No user constraints are added.
This experiment illustrates the execution time savings of eliminating the remapping variables from the model.
Using the linear model, because there are so many decision variables in the model, the majority of modeling time is spent in setup with as few as four loop nests (57\%).
Overall, the nonlinear model is between $1.29\times$ and $2.63\times$ faster than the linear model in this experiment.

\begin{figure}
	\includegraphics[width=0.5\columnwidth]{NestDepth.pdf}
	\caption{(Lack of) Changes in modeling time for different depths of loop nests.}\label{NestDepth}
\end{figure}
The second experiment, shown in Figure~\ref{NestDepth}, shows the modeling time for loop chains where loop nest depth ranges from two to four.
Here, the loop chain contains three loop nests, and all other parameters are the same as the first experiment.
Note that the modeling time is constant across the different nest depths. 
This is because the performance model is data-oriented rather than loop-oriented: it seeks to model the performance of each access in the loop rather than the loop as a whole.
The contribution of each access to the objective function is scaled by the number of iterations.

\begin{figure}
\includegraphics[width=0.5\columnwidth]{Footprint.pdf}
\caption{(Lack of) Changes in modeling time for different memory footprints.}\label{Footprint}
\end{figure}
\todo{Run with sizes based on filling cache.}
The third experiment varies the size of the arrays accessed by the loop chain, shown in Figure~\ref{Footprint}.
Like with the loop nest depth, the modeling time remains constant across the different sizes.
The cause is similar; as the memory footprint increases, the only feature of the model that changes is the magnitude of the coefficients within the objective function.


\begin{figure}
	\includegraphics[width=0.5\columnwidth]{AccessCount.pdf}
	\caption{Changes in modeling time for different numbers of data accesses.}\label{AccessCount}
\end{figure}
\todo{explain the small, linear scaling of the cost estimation, and the negative scaling of the setup in the linear model}
Next is the number of accesses made to the data within the loop body.

\begin{figure}
	\includegraphics[width=0.5\columnwidth]{ConstraintCount.pdf}
	\caption{Changes in modeling time for different numbers of user-provided layout decisions.}\label{ConstraintCount}
\end{figure}
Fifth is the number of layout decisions the user provides. 
Each additional layout decision adds one constraint, reducing the number of valid configurations. 
Thus, the solving time is reduced, as there are fewer points to evaluate.


\begin{figure}
	\includegraphics[width=0.5\columnwidth]{DataDimensionality.pdf}
	\caption{Changes in modeling time for data with different numbers of dimensions.}\label{DataDimensionality}
\end{figure}
Sixth and final is the dimensionality of the data, varying in Figure~\ref{DataDimensionality} from two to six, selected as the maximum for QCD\todo{is this the right thing? i don't remember what bronis said}.
This has the most significant impact on execution time, and without reducing the set of candidate layout using the rotation heuristic, the execution reaches timeout before completing for four, five, and six dimensions.
For the version using the rotation heuristic, the solve time scales linearly with the dimensionality because the number of decision variables increases linearly with the dimensionality.
The cost estimation time scales faster than linearly, \todo{explain}.


\section{Production System}

The \FormatDecisions{} prototype lends itself to an adaptive production system.
This section outlines such a potential system that uses representative training runs to entirely offload the overhead of running the decision model.
The production version uses a single feature not present in the prototype: decision I/O.
Specifically, to use the model results from a training run during a later production run, the system would need to be able to write out and read back in the model's layout decisions to persistent files 
This is not a completely trivial feature, but it is not a significant challenge, and I will return to it after outlining the system's function.

The production system would be run in one of two modes.
In the first mode, meant to be run with smaller but representative inputs, the system runs the decision model for each loop chain it encounters, writing the final, comprehensive layout choices to file.
In the second mode, meant to be used on the full production inputs, the system uses the layout choices produced by the training run.
Because the training run writes the full set of layout decisions for each loop chain, no modeling is necessary and the loop chains can be executed immediately.

Of course, it is not always possible to exercise all code paths with a single training run. 
For this circumstance, I propose a hybrid ``learning'' mode.
Meant to be run with full production inputs, it uses previous layout decision files when available and runs decision models for newly encountered loop chains.
Like the training mode, it writes the decisions to file, so that later runs can reuse the results.
With this scheme, newly encountered code paths will incur modeling costs for a single run, but not thereafter.

Returning to the problem of decision I/O, the key insight is that each \FormatDecisions{} object has a unique type.
Thus, the decisions can be indexed by the hash of its decision object type.
This ensures that decisions for a GPU execution are not used for a CPU execution, as the \FormatDecisions{} object types contain the execution policy types of their constituent kernels.


\section{Evaluation}

I evaluate \FormatDecisions{} against 
\section{Related Work}

\section{Conclusion}

\todo{how to modify this approach for use in kokkos or yakl}
Kokkos's StridedLayout could be used in the same way as RAJA's permuted layouts. 
YAKL is less capable, and is a less mature interface, especially with its container type.






