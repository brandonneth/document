\chapter{Productivity, Performance, Portability}

High performance computer simulation plays a foundational role in modern science and engineering.
For example, considering the nine-figure cost of a utility-scale wind farm~\cite{wiser2022land}, there is significant interest in ensuring the farm will behave as expected. 
High performance computing (HPC), with its focus on the performant execution of applications at this scale, can help provide this confidence.
When developing these types of applications, three interrelated concerns must be balanced. 

First is developer productivity.
From the earliest days of electronic computing~\cite{backus1957fortran} to today, code has been expensive to write.
Every prototype, new feature, or bug fix requires programmer labor, and the productivity of the languages, libraries, and tools they use determines how much.
%While in some circumstances it is economical to use the most productive tools available, this is not always the case.
%In the realm of high performance computing (HPC) for example, applications cannot solely be quick to develop; they must also be quick to run.

The second concern is exactly this: application performance, in terms of time, flops, or watts.
More efficient code mean solving bigger problems, faster and more accurately.
Thus, especially in the realm of HPC, developers expend considerable effort optimizing the performance of their applications, often for the specific architectural configuration of the target system.
The more productive their tools, the easier, faster, and cheaper optimization is.

The third concern, cross-system portability, emerges from the diversity of computing hardware. 
While specialized optimization results in code that uses the target system well, an application tuned for one machine is not guaranteed to perform well on another.
In fact, finding one set of transformations that give good performance across different systems is not always possible.
Differences in cache size, processor, accelerator, or interconnect all influence which version of a code is most performant.
In the extreme, this means developers must maintain multiple versions of each application, one tuned for each system it will be used on.
The code may be performant, but it is not \textit{portably} performant.

In developing HPC applications, the Department of Energy places emphasis on creating codes that are performant on many different systems.
Performance portability libraries like RAJA~\cite{hornung2014RAJA}, Kokkos~\cite{edwards2014kokkos}, and YAKL~\cite{norman2022portable} address this problem by surfacing the performance ``knobs'' that an engineer can use to tune an application for a particular system. 
These include loop schedule transformations --- like loop interchange, tiling, vectorization, OpenMP threading, offloading to accelerators --- and global data format transformations.
While powerful, this approach focuses on tuning each loop independently, leaving out opportunities for optimizations that are applied across loops.
A programmer who wishes to apply valuable cross-loop optimizations like loop fusion~\cite{mckinley1996improving}, overlapped tiling~\cite{bertolacci2019using,zhou2012hierarchical,CathieSC14}, or inter-loop layout changes~\cite{kennedy1995automatic,kennedy1998automatic} cannot do so without breaking the abstractions of the performance portability library.
This sacrifice of portability for performance is also present for a programmer who wishes to express parts of a computation that work with sparse data.
This dissertation seeks to remedy these problems, introducing abstractions to support cross-loop schedule and data transformations and laying the foundation for native support for sparse computations within a representative performance portability library, RAJA\@.
This chapter introduces the programming model of the RAJA library, the limitations of PPLs, and RAJA in particular, and the contributions made in the subsequent chapters. 

\section{Programming with Performance Portability Libraries}

Performance portable libraries (PPLs) are driven by the need to create modularized and easily-tuned code.
This is achieved through a process of decomposing the computation into separable parts.
While the specifics of this decomposition differ across libraries, they share key components.
Broadly, these components appear as data abstractions and loop abstractions.

\begin{figure}[h]
\begin{lstlisting}[caption={C++ reference implementation of a 5-point stencil computation.},label=stencilCpp]
// Initialize and prepare A and B...
std::vector<std::vector<double>> A(N);
std::vector<std::vector<double>> B(N);
for(int i = 0; i < N; i++) {
  std::vector<double> a(M);
  std::vector<double> b(M);
  A[i] = a
  B[i] = b;
}

// Fill with starting conditions, will depend on the problem

// Execute!
for(int i = 1; i < N-1; i++) {
  for (int j = 1; j < M-1; j++) {
    B[i][j] = 0.2 * (A[i][j] + A[i-1][j] + A[i+1][j] + A[i][j-1] + A[i][j+1]);
  }
}
\end{lstlisting}
\end{figure}

Here, I illustrate these abstractions in RAJA using a 5-point stencil computation (see Listings~\ref{stencilRAJA},~\ref{stencilRAJADiff} and~\ref{stencilSchedule}).
Stencil computations are an important class of computation that often appear in image processing and the solution of differential equations.
The example computation uses two $N \times M$ arrays \verb.A. and \verb.B..
A reference implementation in C++ is shown in Listing~\ref{stencilCpp}.
The RAJA implementation in Listing~\ref{stencilRAJA} is longer than the standard C++, but this is not a problem.
Decomposing a computation into individually tunable parts brings benefits that greatly outweigh the cost of a few extra lines of code.
Two of those benefits, per-loop schedule changes and global data layout changes, are the focus of this section.

\begin{figure}
\begin{lstlisting}[caption={Data declaration using RAJA multi-dimensional data abstractions, called Views, followed by the description of a 5-point stencil kernel. Data is initialized to use row-major storage.}, label=stencilRAJA]
// Data Abstractions

std::array<size_t, 2> dimLengths = {N,M};

// Create the layout type...
Layout<2> rowMajor = make_layout(dimLengths, {0,1}); // dim0, then dim1

// Then initialize the Views...
View<double,2> A(new double[N*M], rowMajor); 
View<double,2> B(new double[N*M], rowMajor);

// Loop Abstractions

// Declare a lambda that executes one iteration of the computation...
auto stencilLambda = [=](int i, int j) {
  B(i,j) = 0.2 * (A(i,j) + A(i-1,j) + A(i+1, j) + A(i,j-1) + A(i,j+1));
};

// Create the iteration space...
auto iDimension = RangeSegment(1,N-1);
auto jDimension = RangeSegment(1,M-1);
auto iterationSpace = make_tuple(iDimension,jDimension);

// Define the kernel execution policy...
using SequentialPolicy = KernelPolicy<
  statement::For<0,seq_exec, // Sequential outer loop traverses iDimension
    statement::For<1,seq_exec, // Sequential inner loop traverses jDimension
      statement::Lambda<0> // Calls lambda
>>>;

// Launch!
kernel<SequentialPolicy>(iterationSpace, stencilLambda);
\end{lstlisting}
\end{figure}



\subsection{Data Abstractions}
\begin{figure}
\begin{lstlisting}[caption={Diff for applying global layout transformations in RAJA version of 5-point stencil code. The new code uses column-major instead of row-major.}, label=stencilRAJADiff]
// Data Abstractions

std::array<size_t, 2> dimLengths = {N,M};

// Create the layout type...

<@\hl{-}@> <@\st{Layout<2> rowMajor = make\_layout(dimLengths, \{0,1\}); // dim0, then dim1}@>
<@\hl{+}@> Layout<2> <@\hl{col}@>Major = make_layout(dimLengths, <@\hl{\{1,0\}}@>); // dim1, then dim0

// Then initialize the Views...
<@\hl{-}@> <@\st{View<double,2> A(new double[N*M], rowMajor);}@>
<@\hl{+}@> View<double,2> A(new double[N*M], <@\hl{col}@>Major); 
<@\hl{-}@> <@\st{View<double,2> B(new double[N*M], rowMajor);}@>
<@\hl{+}@> View<double,2> B(new double[N*M], <@\hl{col}@>Major);

// Loop Abstractions

// Declare a lambda that executes one iteration of the computation...
auto stencilLambda = [=](int i, int j) {
  B(i,j) = 0.2 * (A(i,j) + A(i-1,j) + A(i+1, j) + A(i,j-1) + A(i,j+1));
};

// Create the iteration space...
auto iDimension = RangeSegment(1,N-1);
auto jDimension = RangeSegment(1,M-1);
auto iterationSpace = make_tuple(iDimension,jDimension);

// Define the kernel execution policy...
using SequentialPolicy = KernelPolicy<
  statement::For<0,seq_exec, // Sequential outer loop traverses iDimension
    statement::For<1,seq_exec, // Sequential inner loop traverses jDimension
      statement::Lambda<0> // Calls lambda
>>>;

// Launch!
kernel<SequentialPolicy>(iterationSpace, stencilLambda);
\end{lstlisting}
\end{figure}


\begin{figure}
\begin{lstlisting}[caption={Code changes required to switch from row- to column-major order in the C++ implementation of the 5 point stencil.},label=stencilCppColumn]
<@\hl{-} \st{std::vector<std::vector<double>}\st{> A(N);} @>
<@\hl{+}@> std::vector<std::vector<double>> A(<@\hl{M}@>);
<@\hl{-} \st{std::vector<std::vector<double>}\st{> B(N);} @>
<@\hl{+}@> std::vector<std::vector<double>> B(<@\hl{M}@>);

<@\hl{-} \st{for(int i = 0; i < N; i++) \{}@>
<@\hl{+}@> for(int i = 0; i < <@\hl{M}@>; i++) {
<@\hl{-}  \st{std::vector<double> a(M);}@>
<@\hl{+}@>  std::vector<double> a(<@\hl{N}@>);
<@\hl{-}  \st{std::vector<double> b(M);}@>
<@\hl{+}@>  std::vector<double> b(<@\hl{N}@>);
  A[i] = a
  B[i] = b;
}

for(int i = 1; i < N-1; i++) {
  for (int j = 1; j < M-1; j++) {
<@\hl{-}@>    <@\st{B[i][j] = 0.2 * (A[i][j] + A[i-1][j] + A[i+1][j] + A[i][j-1] + A[i][j+1]);} @>
<@\hl{+}@>    B<@\hl{[j][i]}@> = 0.2 * (A<@\hl{[j][i]}@> + A<@\hl{[j][i-1]}@> + A<@\hl{[j][i+1]}@> + A<@\hl{[j-1][i]}@> + A<@\hl{[j+1][i]}@>);
  }
}
\end{lstlisting}
\end{figure}

First, I describe the abstractions in performance portability libraries for multidimensional data.
Then, the layout transformations, like switching between row- and column-major storage, they enable.
Multidimensional data is exceedingly common in simulation codes.
However, a computer's memory is addressable only as a one-dimensional sequence of bytes.
Thus, any multi-dimensional representation must be mapped to the one-dimensional nature of the hardware.
Usually, this is achieved by breaking the data into one-dimensional strips and storing each strip in sequence.
Different ways to break the data into strips produce different orders of elements within memory.
Two examples of these layouts are the well-known 2D column-major and row-major storage formats.
With these layouts, the strips of one-dimensional data are either the columns or the rows of the matrix.

When tuning an application for a new system, changing the layout of data in memory can have large performance implications.
These differences are especially relevant between different processor types, as a data layout that performs well on a CPU is not best for a GPU\@.
It can also impact performance between two CPUs because of threading and cache behaviors~\cite{trott2021kokkos}.

Traditional approaches to representing multi-dimensional data, such as the array of pointers or the vector of vectors used in Listing~\ref{stencilCpp} do not admit an easy data layout change.
This is due to the high degree of coupling between the layout and the method of data access. 
For example, using these approaches, an access to the $(i,j)$th entry of a two dimensional structure will look like this: \verb.A[i][j]..
Should the programmer want to try a column-major ordering instead of the default row-major, their only option is to completely transpose the data structure.
This process starts with the cumbersome but manageable task of re-declaring the sizes of each dimension in the structure's initialization, switching the dimensions' extents (see Listing~\ref{stencilCppColumn}).
But then, every single access to the structure needs to be changed. 
All instances \verb.A[i][j]. must become \verb.A[j][i]..

This can be a significant task, and one where well-camouflaged bugs are born. 
In a paragraph of prose, \verb.[i][j]. is easily distinguished from \verb.[j][i]..
In a complicated loop nest making hundreds of data accesses, all using different orderings of the iterators, one forgotten (or extra) swap quickly creates crashes.
And of course, once the transformation has been applied and the new bugs squashed, it may turn out that a layout change had no effect, or even a negative one.

Called a View in RAJA and Kokkos and an Array in YAKL, multidimensional data structures are a key abstraction through which performance portability libraries surface portability controls to the programmer.
RAJA's View abstraction is shown in action in the first half of Listing~\ref{stencilRAJA}, and in Listing~\ref{stencilRAJADiff}, which shows the code changes necessary to switch layouts.
Because data layout can be changed for the entire program just by changing how the data is declared or initialized, global layout transformations are brought back into the programmer's thought-loop.
Instead of taking hours, the question ``How would this perform with a different data layout?'' is answered in minutes.
These easy changes are made possible by making the layout of the data an explicit parameter to the multi-dimensional data abstractions.
In Kokkos and YAKL, the layout parameter is provided at compile-time, while in RAJA, it is provided as a runtime argument to the View's initializer.
Regardless of where and how the programmer selects the layout specifically, the key idea here is that changing one thing about a data structure only requires changing one thing in the code.

\subsection{Loop Abstractions}

\begin{figure}
\begin{lstlisting}[caption={Kernel policies for different loop schedules.}, label={stencilSchedule}]
using OpenMPwithVectorization = KernelPolicy<
  statement::For<0,omp_parallel_for_exec, //Parallel outer loop traverses iDimension
    statement::For<1,simd_exec, //Vectorized inner loop traverses jDimension
      statement::Lambda<0>
    >
  >
>;

using Interchanged = KernelPolicy<
  statement::For<1,seq_exec, //Sequential outer loop traverses jDimension
    statement::For<0,seq_exec, //Sequential inner loop traverses iDimension
      statement::Lambda<0>
    >
  >
>;

using ParallelTiled = KernelPolicy<
  statement::Tile<0, tile_fixed<64>, omp_parallel_for_exec, //Parallel tiling loop, i
    statement::Tile<1, tile_fixed<64>, seq_exec, //Sequential tiling loop, j
      statement::For<0,seq_exec, //Within tile, iDimension
        statement::For<1,simd_exec, //Within tile vectorized, jDimension
          statement::Lambda<0>
        >
      >
    >
  >
>;
\end{lstlisting}
\end{figure}

\begin{figure}
\begin{lstlisting}[caption={Hand-changed loop schedules for C++ 5-point stencil computation. They are all shown in one figure, but in practice only one would appear within a code at a time.}, label={stencilScheduleCpp}]
// OpenMP with Vectorization
#pragma omp parallel for
for(int i = 1; i < N-1; i++) {
  #pragma omp simd
  for (int j = 1; j < M-1; j++) {
    B[i][j] = 0.2 * (A[i][j] + A[i-1][j] + A[i+1][j] + A[i][j-1] + A[i][j+1]);
  }
}

//Interchanged
for (int j = 1; j < M-1; j++) {
  for(int i = 1; i < N-1; i++) {
    B[i][j] = 0.2 * (A[i][j] + A[i-1][j] + A[i+1][j] + A[i][j-1] + A[i][j+1]);
  }
}

//Parallel Tiled
#pragma omp parallel for
for(int iTile = 0; iTile * 64 < N-1; iTile++) {
  int iLow = max(1, iTile * 64);
  int iHigh = min((iTile+1) * 64, N-1);
  for(int jTile = 0; jTile * 64 < M -1; jTile++) {
    int jLow = max(1, jTile * 64);
    int jHigh = min((jTile+1) * 64, M-1);
    for(int i = iLow; i < iHigh; i++) {
      #pragma omp simd
      for(int j = jLow; j < jHigh; j++) {
        B[i][j] = 0.2 * (A[i][j] + A[i-1][j] + A[i+1][j] + A[i][j-1] + A[i][j+1]);
      }
    }
  }
}
\end{lstlisting}
\end{figure}

As with their data abstractions, performance portability libraries share an approach to their abstractions for specifying and executing loop nests.
At a high level, a loop nest is decomposed into loop bodies describing the operation to do, an iteration space describing the values to do it for, and scheduling information describing how to order and distribute the iterations among the different processing elements.
This process is shown in RAJA for the 5-point stencil computation in the second half of Listing~\ref{stencilRAJA}.

All three performance portability libraries share an approach for representing the operation of an individual loop iteration: the functor.
A \textit{funct}or is just an object that can be called like a \textit{funct}ion, meaning it defines the call operator. 
In practice, these are most often defined using lambda closures, which are unnamed functions that ``capture'' the values of variables used in their definition.
The \verb.stencilLambda. in Listing~\ref{stencilRAJA} is an example of such a lambda closure, and captures the two array Views used in its body.
The parameters of the lambda represent the indices of a point in the iteration space.

The iteration space is specified as a tuple of containers, usually ranges and lists.
In Listing~\ref{stencilRAJA}, the two dimension of the iteration space, \verb.iDimension. and \verb.jDimension. are defined using the \verb.RangeSegment. function.
This function creates a range from the first argument to the second.
Then, the two dimension are grouped into the iteration space tuple.

Last is the schedule for the computation.
In Kokkos, different abstractions are provided for loops with different schedules, including fully parallel loops, reductions, and scans.
In YAKL, a similar approach is used, with the additional of hierarchical parallelism through the \verb.parallel_outer. and \verb.parallel_inner. constructs.
In RAJA, schedules are written using the kernel policy type.
In Listing~\ref{stencilRAJA}, the policy type is \verb.SequentialPolicy..
This policy corresponds to a canonical, sequential, 2-dimensional loop nest.
Changing parts of the kernel policy change the execution schedule.  %good
Listing~\ref{stencilSchedule} shows some of these changes: one for loop interchange, one for OpenMP parallelization and vectorization, and one for loop tiling.
The comparable transformations applied to the C++ implementation are shown in Listing~\ref{stencilScheduleCpp}.

Once these components are specified, they are combined and executed using the \verb.kernel. method, shown on the final line of Listing~\ref{stencilRAJA}.
Changing the loop schedule of the computation is as simple as changing the template argument of the call to a different policy.

\section{Transformation Scope Limitations}

Performance portability libraries provide the ability to apply transformations per-loop and set the data layout per computation.
This guides the programmer towards a common paradigm for program optimization: measure, identify an expensive loop or kernel, try a transformation, and repeat.
Using this paradigm, each kernel is optimized as a standalone piece of code.
While useful for leveraging the maximum possible parallelism from an application, it draws attention away from the interrelation of the kernels.
This means that important opportunities for optimization can go overlooked.
This section reviews three that can lead to significant performance improvement in certain contexts: loop fusion, inter-loop layout transformations, and sparse format transformations.

\subsection{Schedule Changes: Loop Fusion Example}
\begin{figure}
\begin{lstlisting}[caption={Three loops, with and without loop fusion.},label=fusionExample]
//without fusion
for(int i = 0; i < N; i++) {
  A1(i) = A0(i) * 2;
}
for(int i = 0; i < N; i++) {
  A2(i) = A1(i) - 1;
}
for(int i = 1; i < N-1; i++) {
  A3(i) = A2(i-1) / 2;
}

//with fusion
//iterations outside of the shared iteration space
A1(0) = A0(0) * 2;
A1(N-1) = A0(N-1) * 2;
A2(0) = A1(0) - 1;
A2(N-1) = A1(N-1) - 1;

//fused component
for(int i = 1; i < N-1; i++) {
  A1(i) = A0(i) * 2;
  A2(i) = A1(i) - 1;
  A3(i) = A2(i-1) / 2;
}
\end{lstlisting}
\end{figure}

Transformations at the inter-loop scope often target the reuse of data between loops.
In computations bottlenecked by data transfer speeds, like sparse computations and the stencil computation above, these optimizations are especially beneficial. 
Two examples of these optimizations are loop fusion and inter-kernel layout transformations.

Loop fusion is a well-known optimization for improving data locality in sequences of loops that use common data.
It appeared as an automated transformation in source-to-source compilers in the mid 90s~\cite{mckinley1996improving}, but was in use in hand-optimization decades before~\cite{warren1984hierarchical,cocke1971catalogue}.
Modern production C++ compilers can do loop fusion in some simple-to-analyze situations, but function calls and pointer aliasing prevent complete automation.
A simple example of this transformation is shown in Listing~\ref{fusionExample}.
Rather than computing and storing the entirety of the intermediary arrays, loop fusion allows for the values to be used as soon as they are produced, when the value may still even be in the processor's registers.

Unfortunately, this performance benefit is not guaranteed.
As the number of fused loops increases, so does the size of the loop body, and with it the register pressure. 
At a certain point, more values are used in the fused body than exist registers to hold them.
Extras spill into memory, with performance implications.
Because different processors have different register designs, a fusion transformation beneficial for one machine may not be for others.
Furthermore, loop fusion can interfere with the parallelization of the loops being fused.

Newer transformations, like overlapped tiling~\cite{holewinski2012high,krishnamoorthy2007effective}, have been developed to achieve the locality improvements of loop fusion while maintaining sufficient parallelism.
Overlapped tiling does this by introducing small amounts of recomputation.
In this transformation, each tile overlaps with those next to it, sharing some points of the iteration space.
By choosing tile sizes that keep the fraction of recomputation low, parallel execution of the tiles still benefits from improved locality.
This transformation has been incorporated into an OpenCL compiler by Zhou et al~\cite{zhou2012hierarchical} and Polymage~\cite{mullapudi2015polymage}, a DSL for image processing pipelines, incorporates overlapped tiling into its automatic optimization passes.
These approaches do not allow for programmer control over the tiling process.
Halide~\cite{ragan-kelley2013halide}, another DSL for image processing pipelines, surfaces control over tiling to the programmer.
For the optimization of existing codes, Bertolacci et al~\cite{bertolacci2019using} developed an approach based on pragma directives.

While the fused code shown in Listing~\ref{fusionExample} is not appreciably more complex than the original sequence of kernels, it does require breaking some of the loop iterations out of the loop nest.
This makes it more difficult to parse exactly what the code is doing and why parts of it are in the loop and others are not.
With overlapped tiling, this is amplified by the tile calculations that obscure which parts of the code are the operation and which parts are the scheduling. 

\subsection{Layout Changes: Transpose Example}
\begin{figure}
\begin{lstlisting}[caption={Changing a View's data layout between computations by hand.},label={FormatChangeByHand}]
//C = A * B
auto knl1 = make_kernel<KPOL>(segs1, [=](auto i0, auto i1, auto i2) {
	C(i0, i1) += A(i0, i2) * B(i2, i1);
});

//E = A^T * D
auto knl2 = make_kernel<KPOL>(segs2, [=](auto i0, auto i1, auto i2) {
	E(i0, i1) += A(i2, i0) * D(i2, i1);
});

knl1();

// Reorder A's data for column-major order
double * tmp1 = new double[n*n];
for(int i = 0; i < n; i++) {
	for(int j = 0; j < n; j++){
		tmp1[j*n + i] = A(i,j);
	}
}
memcpy(A.data, tmp1, n*n);
A.layout = colMajor;
delete[] tmp1;

knl2();
\end{lstlisting}
\end{figure}
Like inter-loop schedule transformations, inter-loop layout transformations can improve the performance of some sequences of loop nests.
If two loop nests access the same data in different orders, it can be worthwhile to change how that data is organized in memory so that both loop nests access the data in the most efficient way possible.
As a simple example, consider the matrix multiplication $C = A * B$ followed by the matrix multiplication $E = A^{\top} * D$. 
The way these loop nests will traverse the matrix $A$ is different. 
In the computation of $C$, the data of $A$ is traversed across the rows, while in the computation of $E$, it is traversed across the columns.
Assume for a moment that $A$ is stored in row-major order.
Because the $C$ computation traverses the data of $A$ by rows, adjacent iterations will access elements of $A$ that are adjacent in memory.
This has much more favorable cache behavior than the computation of $E$, where adjacent iterations access elements of $A$ that are more distant in memory.
Performance can be improved by changing the layout of $A$ between the two loop nests, ensuring that both have favorable access patterns.
However, applying such a transformation by hand is, as discussed above, time consuming and error prone. 
An example of such a manual implementation of this transformation is shown in Listing~\ref{FormatChangeByHand}.
While PPLs do support changing the layout of a View at a global scope, they do not support making these changes between loop nests. 

Early work on the problem of automatically selecting data layouts were done in the High Performance Fortran and D programming languages using graph theoretic formulations, specifically for distributed computing systems~\cite{kennedy1995automatic,kennedy1998automatic}.
Later work reformulated the decision problem as constraint networks~\cite{chen2005constraint}.
With the rise of multicore and vector processors, new approaches to transformations were developed~\cite{lu2009data,henretty2011data,zhang2011optimizing}.
Jaeger and Barthou developed an automated approach to transformation selection for stencil computations on GPUs~\cite{jaeger2012automatic}.
More recently, Kronawitter et al incorporated layout transformations into the ExaStencil DSL, providing a balance of user control and automation~\cite{kronawitter2018automatic}.
Because these works have all been parts of restructuring compilers or domain specific languages, they cannot be used directly to transform codes written with PPLs.

\subsection{Sparse Computations}

The previous subsection discusses changing dense data layouts.
Sparse computations, meaning computations that operate on data containing mostly zeros, are both critical to scientific computing and benefit especially from data movement optimizations.
Because data in these computations is mostly empty, different storage schemes are used to avoid storing the zero values explicitly. 
These storage approaches use compression and abstraction strategies that constrain accesses to the data.
This means that sparse computations are more strongly memory-bound than dense computations, as more cycles are needed to find and move data around.

Sparse formats abound~\cite{langr2015evaluation}. 
While there are general purpose formats like coordinate storage (COO) and compressed sparse row (CSR), formats vary widely based on the characteristics of the data, the hardware, and the algorithm.
This variety leads to highly inflexible code, where the format affects every part of the computation description.
Approaches to this problem usually rely on describing a computation at a higher level that abstracts away the format being used.
Then, using code generation, this representation is lowered into an implementation for a given format.
The specifics vary by approach.
The Bernoulli group's work uses a C++ template based approach, with one interface for describing computations~\cite{kotlyar1997relational} and one for describing formats~\cite{kotlyar1997compiling}.
While based on generic programming, their approach does rely on a restructuring compiler to produce efficient code~\cite{mateev2000bernoulli,ahmed2000framework}.
Other approaches restrict the space of expressible computations, like the tensor algebra compiler~\cite{kjolstad2017tensor}.
By restricting the computation space to tensor algebra expressions, their approach is able to generate efficient implementations for a wide variety of formats from a high level computation description.
Finally, the sparse polyhedral framework~\cite{strout2016approach} allows for a variety of composable optimizing transformations on sparse codes~~\cite{ahmad2017optimizing}.
Recent work has enabled the synthesis of transformations among different formats~\cite{popoola2023code}, and a code description that decouples the format from the computation~\cite{zhao2022polyhedral}.

While PPLs give limited support for representing sparse computations, they are generally not performance portable, especially when changing the format of the sparse data.
The execution model requires sparse computations to be flattened to a single loop that iterates over all nonzeros. 
For some computations, like the perfectly nested sparse matrix vector multiplication, this is acceptable.
For others, like the imperfectly nested Gauss-Seidel iteration, flattening the loop nest is a challenge.
This problem is exacerbated if a new data format is to be used, as the entire loop body must be rewritten.
It is desirable to have a single interface for sparse computations that supports different sparse formats and maintains the multidimensional loop and data abstractions.

\section{Summary of Contributions and Code Examples}

This dissertation aims to strengthen the performance portability library approach in three ways: support for cross-kernel schedule transformations, cross-kernel data transformations, and sparse computations.
I develop these capabilities within a representative performance portability library, RAJA\@, so many of the challenges are specific to RAJA's programming model.
Using similar approaches in other PPLs would present different challenges in implementation, but the overall strategy would likely be similar.
I selected RAJA for pragmatic reasons, but also because it has a wider range of scheduling constructs than Kokkos or YAKL\@.

In Chapter~\ref{chap:RAJALC}, I introduce RAJALC, which provides support for cross-kernel schedule transformations.
Two challenges that arise here are the need for an interface for manipulating computations and the need for information about how the computations access their data.
To solve these problems, I introduce two extensions to the RAJA library that are used throughout the dissertation.
First are kernel wrappers that turn RAJA computations into objects that can be manipulated by the programmer.
Second is the symbolic evaluation of kernel bodies, which collects data access information used to verify transformation safety and guide the automation of optimizations.
Using these, I introduce two cross-kernel schedule transformations: loop fusion and overlapped tiling. 
Listing~\ref{Intro:transformExample} displays the use of the RAJALC framework I developed.
For loop fusion, using RAJALC requires about as many code changes as implementing the transformation by hand.
However, RAJALC maintains a stronger separation of computation and schedule than hand-implemented versions.
For overlapped tiling, the improvement is much more significant.
On average, RAJALC requires about a quarter as many code changes while achieving up to 98\% of the performance improvement of a hand-implemented transformation.
\begin{figure}[t]
\begin{lstlisting}[caption={Using the \texttt{fuse} and \texttt{overlapped\_tile} transformations.}, label={Intro:transformExample}]
auto lambda1 = [=](auto i) {c(i) = a(i) * b(i);};
auto lambda2 = [=](auto i) {d(i) = d(i) + c(i);};

auto loop1 = make_forall<simd_exec>(RangeSegment(0,N), lambda1);
auto loop2 = make_forall<simd_exec>(RangeSegment(0,N), lambda2);

auto fused = fuse(loop1, loop2);
fused();

auto lambda3 = [=](auto i) {b(i) = (a(i-1) + a(i) + a(i+1) ) / 3;};
auto lambda4 = [=](auto i) {c(i) = (b(i-1) + b(i) + b(i+1) ) / 3;};

auto loop3 = make_forall<...>(RangeSegment(1,N-1), lambda3);
auto loop4 = make_forall<...>(RangeSegment(1,N-1), lambda4);

//default tile size
auto tiled_default = overlapped_tile(loop3, loop4);
//tile size = 64
auto tiled_64 = overlapped_tile<64>(loop3, loop4);
tiled_64();
\end{lstlisting}
\end{figure}

In Chapter~\ref{chap:FormatDecisions}, I build on the kernel wrapper and symbolic evaluation extensions to introduce inter-kernel data layout transformations through the \FormatDecisions{} interface.
Further, I incorporate an automated decision model based on runtime profiling to identify profitable layout transformations without user intervention.
I describe how this system could be incorporated into a production codebase to reuse modeling results, further amortizing modeling costs across many runs of a program.
The first challenge here is providing user control over the transformations without sacrificing the benefits of automated transformation.
To address this, the interface allows the programmer to succinctly specify the desired format for different pieces of data throughout a computation.
Then, because I incorporate a runtime performance model into the system, it can identify potentially beneficial transformations in addition to those specified by the programmer.
Overall, this creates an as-automated-as-desired system built directly into the library.
The second challenge is representing computations with triangular iteration spaces.
I augment the interface for describing iteration spaces in RAJA to iteration spaces that are not hyper-rectangular.
An example code using \FormatDecisions{} is shown in Listing~\ref{Intro:FormatDecisions3MM}.
Like with RAJALC, much less code is required to implement layout changes.
Instead of managing the reorganization of data themselves, the programmer simply registers the formats the Views should have for different computations and the system manages applying the transformations.

\begin{figure}
\begin{lstlisting}[caption={Changing data layouts for three Views in the \textsc{3mm} benchmark using \FormatDecisions.},
  label={Intro:FormatDecisions3MM}]
auto knl1 = make_kernel<KPOL>(segs1, [=](auto i0, auto i1, auto i2) {
  E(i0, i1) += A(i0, i2) * B(i2, i1);
});
auto knl2 = make_kernel<KPOL>(segs2, [=](auto i0, auto i1, auto i2) {
  F(i0, i1) += C(i0, i2) * D(i2, i1);
});
auto knl3 = make_kernel<KPOL>(segs3, [=](auto i0, auto i1, auto i2) {
  G(i0, i1) += E(i0, i2) * F(i2, i1);
});

auto decisions = format_decisions(ref_tuple(B,D,F), knl1, knl2, knl3);

decisions.set_format_for(B, {{1,0}}, knl1); // column-major
decisions.set_format_for(D, {{1,0}}, knl2); // column-major
decisions.set_format_for(F, {{0,1}}, knl2); // row-major
decisions.set_format_for(F, {{1,0}}, knl3); // column-major

// Generate and run the computations with format conversions
auto computation = decisions.generate();
computation();
\end{lstlisting}
\end{figure}

Finally, in Chapter~\ref{chap:SparseRAJA} I develop prototype support for representing format-agnostic sparse computation and data within RAJA\@.
The two challenges for sparse computations are constructing the sparse iteration space and efficiently iterating through the sparse data.
Using symbolic evaluation to gather the data access patterns in the computation, the prototype constructs a sparse iteration space from the dense-like specification provided by the programmer.
Then, for iterating through sparse data, the system uses a software prefetching approach during kernel execution to reduce the computational complexity of data access.
Overall, this creates a programming interface that looks like dense code, but operates on sparse data.
Listing~\ref{Intro:SparseRAJAMV} shows an implementation of sparse matrix vector multiplication using the prototype.
While algorithm complexity suggests that performance could compare to format-specialized implementations, runtime overhead was significant.
I identify sources of overhead and fundamental limitations to the approach.
The overhead comes from two sources.
First, the structures used to represent and traverse the sparse iteration space use virtualization to meet type requirements of the RAJA execution model, incurring devirtualization overhead.
For perfectly nested loops, the traversal can be restructured to remove this overhead, but comes at the cost of the ability to express imperfectly nested loops.
Second, the prefetching approach introduces expensive conditionals into the innermost loop.
For loops with simple access patterns, specialized iterator types could bypass these conditionals.
However, such an approach would require the code use only RAJA View types and could not support loops that do not traverse the data in order.
I conclude that to efficiently support general sparse computations with dependences, more significant changes to the performance portability library interface will be needed.
Overall, the existing abstractions are capable of supporting dense cross-loop schedule and data transformations and sparse format transformations for a limited subset of sparse codes.

\begin{figure}
\begin{lstlisting}[caption={Implementation of SpMV using the SparseRAJA prototype},label=Intro:SparseRAJAMV]
DenseView<1> x(Nj);
DenseView<1> y(Ni);
SparseView<2> A(Ni,Nj);

using POLICY = KernelPolicy<
  statement::For<0,loop_exec,
    statement::For<1,loop_exec,
      statement::Lambda<0>
    >
  >
>;

auto seg1 = RangeSegment(0,Ni);
auto seg2 = RangeSegment(0,Nj);
auto dense_segs = make_tuple(seg1, seg2);

auto lam = [&](auto i, auto j) {
  y(i) += A(i,j) * x(j);
}

auto knl = make_sparse_kernel<POLICY>(dense_segs, A, lam);
  
knl();
\end{lstlisting}
\end{figure}

The code developed as part of this dissertation can be found at github.com/brandonneth/RAJA on branches \verb.loopchain., \verb.formatdecisions., and \verb.sparseRAJA. for chapters 2, 3, and 4, respectively.
