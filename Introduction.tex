\chapter{Introduction}

High performance computer simulation plays a foundational role in modern science and engineering.
For example, considering the ten-figure cost of a utility-scale wind farm~\cite{farmCost}, there is significant interest in ensuring the farm will behave as expected. 
Computer simulation can help provide this confidence.

When developing these types of applications, three competing concerns need balanced. 
First is developer productivity.
From the earliest days of electronic computing~\cite{backus1957fortran} to today, code has been expensive to write.
High-productivity languages like Python excel in the realm of developer productivity.
While the actual execution of the code is slow, the ease of writing, maintaining, and updating the code can make up for the slow performance.
However, in many situations, such as HPC, the code also needs to run quickly.
The second competing concern is exactly this: application performance.
There are often real-world constraints on how long code execution can take.
Consider the automatic braking system in many modern vehicles.
These systems do not have the luxury of running slowly, else they risk the safety of their passengers.
On the other end, large simulation codes need to make the most out of the limited time they are allotted on HPC systems.
Writing fast code runs at odds with programmer productivity in part because achieving good application performance normally requires the use of languages like C++ and Fortran, languages rarely lauded for developer productivity.
The final competing concern, cross-system portability, runs in tension with both application performance and programmer productivity.
Computing hardware is diversifying, meaning code optimized for one system may perform poorly on another.
Finding one set of optimizations that give good performance across different systems is not straightforward, and not even always possible.
The alternative is to maintain multiple versions of the same code base, each specially optimized for the system it is designed for.

In developing HPC applications, DoE places emphasis on creating codes that run performantly on many different systems.
Performance portability libraries like RAJA~\cite{hornung2014RAJA}, Kokkos~\cite{edwards2014kokkos}, and YAKL~\cite{norman2022portable} address this problem by surfacing the performance ``knobs'' that an engineer can use to tune an application for a particular system. 
These include loop schedule transformations --- like loop interchange, tiling, vectorization, OpenMP threading, offloading to accelerators --- and global data format transformations.
While powerful, this approach focuses on tuning each loop independently, leaving out opportunities for optimizations that are applied across loops.
A programmer who wishes to apply valuable cross-loop optimizations like loop fusion~\cite{todo}, overlapped tiling~\cite{bertolacci2019using,todoCathySCPaper, todoOVerlappedTilingIntroPaper}, or inter-loop layout changes~\cite{kennedy1995automatic,kennedy1998automatic} cannot do so without breaking the abstractions of the performance portability library.
This sacrifice of portability for performance is also present for a programmer who wishes to express parts of a computation that work with sparse data.
This dissertation seeks to remedy this problem, introducing abstractions to support cross-loop schedule and data transformations and laying the foundation for native support for sparse computations within the RAJA library.

\section{Programming with a Performance Portability Libraries}
Performance portabile libraries are driven by the need to create modularized and easily-tuned code.
This is achieved through a process of decomposing the computation into seperable parts.
While the specifics of this decomposition differ across libraries, they share key components.
Broadly, these components appear as data abstractions and operation/execution abstractions.
Here, I demonstrate these abstractions using a 5-point stencil computation, broken into Listings~\ref{stencilData},~\ref{stencilKernel}, and~\ref{stencilSchedule}.
This computation uses two $N \times M$ arrays \verb.A. and \verb.B..


\subsubsection{Data Abstractions}
\begin{figure}
\begin{lstlisting}[caption={Data declaration using RAJA multi-dimensional data abstractions, called Views. Changing a View's layout only requires changing the second argument in its constructor.}, label=stencilData]
using namespace RAJA;

//Create an array with the dimension extents...
std::array<size_t, 2> dimLengths = {N,M};

//Create the layout type...
auto rowMajor = make_layout(dimLengths, {0,1}); //dim0, then dim1
auto colMajor = make_layout(dimLengths, {1,0}); //dim1, then dim0

//Then initialize the Views...

//with row-major storage
View<double,2> A(new double[N*M], rowMajor);
View<double,2> B(new double[N*M], rowMajor);

//OR

//with column-major storage
View<double,2> A(new double[N*M], colMajor);
View<double,2> B(new double[N*M], colMajor);
\end{lstlisting}
\end{figure}

Multi-dimensional data is exceedingly common in simulation codes.
However, a computer's memory is addressable only as a one-dimensional sequence of bytes.
Thus, any multi-dimensional representation must be mapped to the 1-dimesional nature of the hardware.
Usually, this is achieved by breaking the data into one-dimensional strips and storing each strip in sequence.
Different ways to break the data into strips produce different orders of elements within memory.
Two of these layouts that are well-known are the two dimensional column-major and row-major storage formats.
Here, the strips of two-dimensional data are either the columns or the rows.

Called a View in RAJA and Kokkos and an Array in YAKL, multidimensional data structures are a key abstraction through which these embedded libraries surface portability controls to the programmer.
When tuning an application for a new system, changing the layout of data in memory can have large performance implications.
These differences are especially relevant between different processor types, where a data layout that performs well on a CPU is not best for a GPU\@.
It can also impact performance between two CPUs because of threading and cache behaviors~\cite{trott2021kokkos}.


Traditional approaches to representing multi-dimensional data, such as the array of pointers or vector of vectors, do not admit an easy data layout change.
This is due to the high degree of coupling between the layout and the method of data access. 
For example, using these approaches, an access to the $(i,j)$th entry of a two dimensional structure will look like this: \verb.A[i][j]..
Should the programmer want to try a column-major ordering instead of the default row-major, their only option is to completely transpose the data structure.
This process starts with the cumbersome but managable task of redeclaring the sizes of each dimension in the structure's initializiation, switching the dimensions' extents.
Then, every single access to the structure needs to be changed. 
All instances \verb.A[i][j]. must become \verb.A[j][i]..
This is a enormous task, and one where well-camouflaged bugs are born. 
In a paragraph of prose, \verb.[i][j]. is easily distinguished from \verb.[j][i]..
In a complicated loop nest making hundreds of data accesses, all using different orderings of the iterators, one forgotten (or extra) swap quickly creates crashes.
And of course, once the transformation has been applied and the new bugs squashed, it may turn out that a layout change had no effect, or even a negative one.

The multi-dimensional abstractions in P3 libraries bring the cost of such a transformation back to the scale of the programmer's thought-loop by introducing layout polymorphism.
In Kokkos and YAKL, this polymorphism is restricted to compile-time.
In RAJA, this polymorphism is restricted to runtime at the point of initialization.
Regardless of where and how the programmer selects the layout specifically, the key idea here is that changing one thing about a data structure only requires changing one thing in the code.
Listing~\ref{stencilData} shows the process of initializing two Views and highlights the minimal code changes required to switch between different layouts.

\subsubsection{Operation/Execution Abstractions}
\begin{figure}
\begin{lstlisting}[caption={Specification of a 5-point stencil kernel using RAJA.}, label=stencilKernel]
//Declare a lambda that executes one iteration of the computation...
auto stencilLambda = [=](int i, int j) {
  B(i,j) = 0.2 * (A(i,j) + A(i-1,j) + A(i+1, j) + A(i,j-1) + A(i,j+1));
};

//Create the iteration space...
auto iDimension = RangeSegment(1,N-1);
auto jDimension = RangeSegment(1,M-1);
auto iterationSpace = make_tuple(iDimension,jDimension);

//Define the kernel execution policy...
using SequentialPolicy = KernelPolicy<
  statement::For<0,seq_exec, //Sequential outer loop traverses iDimension
    statement::For<1,seq_exec, //Sequential inner loop traverses jDimension
      statement::Lambda<0> //Calls lambda
    >
  >
>;

//Launch!
kernel<SequentialPolicy>(iterationSpace, stencilLambda);
\end{lstlisting}
\end{figure}

As with their data abstractions, P3 libraries share an approach to their abstractions for specifying and executing the operations of a computation.
At a high level, the computation is decomposed into an operation describing what to do, an iteration space describing which values to do it for, and scheduling information describing how to order and distribute them among the different processing elements.
This process is shown for a 5-point stencil computation in Listing~\ref{stencilKernel}.

All three libraries share their use of functors to represent the operation of an individual iteration.
Often represented using a lambda closure, these loop body statements form the basis for the description of a computation.
Parameters to the lambdas represent the indices of an iteration space point. 
An example of such a lambda is shown on lines 1--4 of Listing~\ref{stencilKernel}.

The iteration space is specified as a tuple of containers.
In Listing~\ref{stencilKernel}, lines 6--9 show this process for the 5-point stencil. 
The \verb.RangeSegment. function creates a range from its first argument to its second.

Last is the schedule for the computation.
In RAJA, this comes in the form of the kernel policy.
Lines 11--18 of Listing~\ref{stencilKernel} show the policy that describes the canonical sequential schedule.
Changing parts of the kernel policy change the execution schedule. 
Listing~\ref{stencilSchedule} shows some of these changes: one for loop interchange, one for OpenMP parallelization and vectorization, and one for loop tiling.

Once these components are specified, they are combined and executed using the \verb.kernel. method, shown on the final line of Listing~\ref{stencilKernel}.
Changing the loop schedule of the computation is as simple as changing the template argument to the call to a different policy.


\begin{figure}
\begin{lstlisting}[caption={Kernel policies for different loop schedules.}, label={stencilSchedule}]
using OpenMPwithVectorization = KernelPolicy<
  statement::For<0,omp_parallel_for_exec, //Parallel outer loop traverses iDimension
    statement::For<1,simd_exec, //Vectorized inner loop traverses jDimension
      statement::Lambda<0>
    >
  >
>;

using Interchanged = KernelPolicy<
  statement::For<1,seq_exec, //Sequential outer loop traverses jDimension
    statement::For<0,seq_exec, //Sequential inner loop traverses iDimension
      statement::Lambda<0>
    >
  >
>;

using ParallelTiled = KernelPolicy<
  statement::Tile<0, tile_fixed<64>, omp_parallel_for_exec, //Parallel tiling loop
    statement::Tile<1, tile_fixed<64>, seq_exec, //Sequential tiling loop
      statement::For<0,seq_exec, //Within tile, iDimension
        statement::For<1,simd_exec, //Within tile, jDimension
          statement::Lambda<0>
        >
      >
    >
  >
>;
\end{lstlisting}
\end{figure}

\section{Optimization Scope}

The transformations that these libraries surface to the programmer support a common paradigm for program optimization: profile, identify an expensive loop or kernel, try a transformation, and repeat.
Using this paradigm, each kernel is optimized as a standalone piece of code.
While useful for leveraging the maximum possible parallelism from an application, it draws attention away from the interrelation of the kernels.
This means that important opportunities for performance optimization can go overlooked.

\subsection{Inter-Loop Transformations}
Transformations at the inter-loop scope often target the reuse of data between loops.
In computations bottlenecked by data transfer speeds, like sparse computations and the stencil computation above, these optimizations are especially beneficial. 
Two examples are overlapped tiling and inter-kernel layout transformations.

\subsubsection{Loop Fusion}
\begin{figure}
\begin{lstlisting}[caption={Three loops, with and without loop fusion.},label=fusionExample]
//without fusion
for(int i = 0; i < N; i++) {
  A1(i) = A0(i) * 2;
}
for(int i = 0; i < N; i++) {
  A2(i) = A1(i) - 1;
}
for(int i = 1; i < N-1; i++) {
  A3(i) = A2(i-1) / 2;
}

//with fusion
//iterations outside of the shared iteration space
A1(0) = A0(0) * 2;
A1(N-1) = A0(N-1) * 2;
A2(0) = A1(0) - 1;
A2(N-1) = A1(N-1) - 1;

//fused component
for(int i = 1; i < N-1; i++) {
  A1(i) = A0(i) * 2;
  A2(i) = A1(i) - 1;
  A3(i) = A2(i-1) / 2;
}
\end{lstlisting}
\end{figure}

Loop fusion is a well-known optimization for improving data locality, appearing as an automated transformation in source to source compilers in the mid 90s~\cite{mckinley1996improving}, but was in use decades before~\cite{warren1984hierarchical,thomas1971catalogue}.
A simple example of this transformation is shown in Listing~\ref{fusionExample}.
Rather than computing and storing the entirity of the intermediary arrays, loop fusion allows for the values to be used as soon as they are produced, when the value may still even be in the CPU's registers.
However, loop fusion can interfere with the parallelization of the loops being fused.
Newer transformations, like overlapped tiling, have been developed to achieve the locality improvements of loop fusion while maintaining sufficient parallelism.

Overlapped tiling~\cite{holewinski2012high,krishnamoorthy2007effective} maintains parallelism by introduces small amounts of recomputation.
In this transformation, each tile overlaps with those next to it, sharing some points of the iteration space.
By choosing tile sizes that keep the fraction of recomputation low, parallel execution of the tiles still benefits from improved locality.
This transformation has been ncorporated into an OpenCL compiler by Zhou et al~\cite{zhou2012hierarchical} and Polymage~\cite{mullapudi2015polymage}, a DSL for image processing pipelines, incorporates overlapped tiling into its automatic optimization passes.
These approaches do not allow for programmer control over the tiling process.
Halide~\cite{ragan-kelley2013halide}, another DSL for image processing pipelines, surfaces control over tiling to the programmer.
For the optimization of existing codes, Bertolacci et al~\cite{bertolacci2019using} develop an approach based on pragma directives.

\subsubsection{Layout Changes}
\begin{figure}
\begin{lstlisting}[caption={Changing a View's data layout between computations by hand.},label={FormatChangeByHand}]
//C = A * B
auto knl1 = make_kernel<KPOL>(segs1, [=](auto i0, auto i1, auto i2) {
	C(i0, i1) += A(i0, i2) * B(i2, i1);
});

//E = A^T * D
auto knl2 = make_kernel<KPOL>(segs2, [=](auto i0, auto i1, auto i2) {
	E(i0, i1) += A(i2, i0) * D(i2, i1);
});

knl1();

// Reorder A's data for column-major order
double * tmp1 = new double[n*n];
for(int i = 0; i < n; i++) {
	for(int j = 0; j < n; j++){
		tmp1[j*n + i] = A(i,j);
	}
}
memcpy(A.data, tmp1, n*n);
A.layout = colMajor;
delete[] tmp1;

knl2();
\end{lstlisting}
\end{figure}

If two computations access the same data in different orders, it can be worthwhile to change how that data is organized in memory so that both computations access the data in the most efficient way possible.
As a simple example, consider the matrix multiplication $C = A * B$ followed by the matrix multiplication $E = A^{\top} * D$. 
The way these computations will traverse the matrix $A$ is different. 
In the computation of $C$, the data of $A$ is traversed across the rows, while in the computation of $E$, it is traversed across the columns.
Assume for a moment that $A$ is stored in row-major order.
Because the $C$ computation traverses the data of $A$ by rows, adjacent iterations will access elements of $A$ that are adjacent in memory.
This has much more favorable cache behavior than the computation of $E$, where adjacent iterations access elements of $A$ that are more distant in memory.
Performance can be improved by changing the layout of $A$ between the two computations, ensuring that both kernels have favorable access patterns.
However, applying such a transformation by hand is, as discussed above, time consuming and error prone. 
An example of such a manual implementation of this transformation is shown in Listing~\ref{FormatChangeByHand}.

Early work on the problem of automatically selecting data layouts were done in the High Performance Fortran and D programming languages using graph theoretic formulations, specifically for distributed computing systems\cite{kennedy1995automatic,kennedy1998automatic}.
Later work reformulated the decision problem as constraint networks\cite{chen2005constraint}.
With the rise of multicore and vector processors, new approaches to transformations were developed~\cite{lu2009data,henretty2011data,zhang2011optimizing}.
Jaeger and Barthou developed an automated approach to transformation selection for stencil computations on GPUs~\cite{jaeger2012automatic}.
More recently, Kronawitter et al incorporated layout transformations into the ExaStencil DSL, providing a balance of user control and automation~\cite{kronawitter2018automatic}.

\subsection{Sparse Computations}

Sparse computations, meaning computations that operate on data containing mostly zeros, are both critical to scientific computing and benefit especially from data movement optimizations.
Because data in these computations is mostly empty, different storage schemes are used to avoid storing the zero values explicitly. 
These storage approaches utilize compression and abstraction strategies that constrain accesses to the data.
This means that sparse computations are more strongly memory-bound than dense computations, as more cycles are needed to find and move data around.

Sparse formats abound~\cite{langr2015evaluation}. 
While there are general purpose formats like coordinate storage (COO) and compressed sparse row (CSR), formats vary widely based on the characteristics of the data, the hardware, and the algorithm.
This variety leads to highly inflexible code, where the format affects every part of the computation description.
Approaches to this problem usually rely on describing a computation at a higher level that abstracts away the format being used.
Then, using code generation, this representation is lowered into an implementation for a given format.
The specifics vary by approach.
The Bernoulli group's work uses a C++ template based approach, with one interface for describing computations~\cite{kotlyar1997relational} and one for describing formats~\cite{kotlyar1997compiling}.
While based on generic programming, their approach does rely on a restructuring compiler to produce efficient code~\cite{mateev2000bernoulli,ahmed2000framework}.
Other approaches restrict the space of expressible computations, like the tensor algebra compiler~\cite{kjolstad2017tensor}.
By restricting the computation space to tensor algebra expressions, their approach is able to generate efficient implementations for a wide variety of formats from a high level computation description.
Finally, the sparse polyhedral framework~\cite{strout2016approach} allows for a variety of composable optimizing transformations on sparse codes~~\cite{ahmad2017optimizing}.
Recent work has enabled the synthesis of transformations among different formats~\cite{popoola2023code}, and a code description that decouples the format from the computation~\cite{zhao2022polyhedral}.

RAJA, and performance portability libraries more generally, cannot in many cases represent sparse computations at all, let alone in a performance portable way.
This means that a scientific applications can use RAJA for parts of its implementation, but must rely on other approaches to sparse computation for those parts.
It is desirable for performance portability libraries to support the description of a wider range of computations, and this dissertation lays the foundations for that support.

\section{Summary of Contributions}

This dissertation aims to strengthen the P3 library approach in three ways: support for cross-kernel schedule transformations, cross-kernel data transformations, and sparse computations.
Each of these contributions add this support through an interface that reduce the labor requirements of their use while maintaining the performance improvements of the optimizations they provide.

In Chapter~\ref{chap:RAJALC}, I introduce support for cross-kernel schedule transformations.
Two challenges that arise here are the need for an interface for manipulating computations and the need for information about how the computations access their data.
To solve these problems, I introduce two extensions to the RAJA library that are used throughout the dissertation.
First are kernel wrappers that turn RAJA computations into objects that can be manipulated by the programmer.
Second is the symbolic evaluation of kernel bodies, which collects data access information used to verify transformation safety and guide the automation of optimizations.
Using these, I introduce two cross-kernel schedule transformations: loop fusion and overlapped tiling.
\begin{figure}[t]
\begin{lstlisting}[caption={Using the \texttt{fuse} and \texttt{overlapped\_tile} transformations.}, label={Intro:transformExample}]
auto lambda1 = [=](auto i) {c(i) = a(i) * b(i);};
auto lambda2 = [=](auto i) {d(i) = d(i) + c(i);};

auto loop1 = make_forall<simd_exec>(RangeSegment(0,N), lambda1);
auto loop2 = make_forall<simd_exec>(RangeSegment(0,N), lambda2);

auto fused = fuse(loop1, loop2);
fused();

auto lambda3 = [=](auto i) {b(i) = (a(i-1) + a(i) + a(i+1) ) / 3;};
auto lambda4 = [=](auto i) {c(i) = (b(i-1) + b(i) + b(i+1) ) / 3;};

auto loop3 = make_forall<...>(RangeSegment(1,N-1), lambda3);
auto loop4 = make_forall<...>(RangeSegment(1,N-1), lambda4);

//default tile size
auto tiled_default = overlapped_tile(loop3, loop4);
//tile size = 64
auto tiled_64 = overlapped_tile<64>(loop3, loop4);
tiled_64();
\end{lstlisting}
\end{figure}

In Chapter~\ref{chap:FormatDecisions}, I build on the kernel wrapper and symbolic evaluation extensions to introduce inter-kernel data layout transformations through the \FormatDecisions{} interface.
The challenge here is balancing user control of the transformations applied with the ease of automated transformations.
To address this, the interface allows the programmer to succinctly specify the desired format for different pieces of data throughout a computation.
Then, because I incorporate a runtime performance model into the system, it can identify potentially beneficial transformations in addition to those specified by the programmer.
Overall, this creates an as-automated-as-desired system, built directly into the library.
Additionally, I augment the interface for describing iteration spaces in RAJA to support non-hyperrectangular iteration spaces.
\begin{figure}
\begin{lstlisting}[caption={Changing data layouts for three Views in the \textsc{3mm} benchmark using \FormatDecisions.},
  label={Intro:FormatDecisions3MM}]
auto knl1 = make_kernel<KPOL>(segs1, [=](auto i0, auto i1, auto i2) {
  E(i0, i1) += A(i0, i2) * B(i2, i1);
});
auto knl2 = make_kernel<KPOL>(segs2, [=](auto i0, auto i1, auto i2) {
  F(i0, i1) += C(i0, i2) * D(i2, i1);
});
auto knl3 = make_kernel<KPOL>(segs3, [=](auto i0, auto i1, auto i2) {
  G(i0, i1) += E(i0, i2) * F(i2, i1);
});

auto decisions = format_decisions(ref_tuple(B,D,F), knl1, knl2, knl3);

decisions.set_format_for(B, {{1,0}}, knl1); // column-major
decisions.set_format_for(D, {{1,0}}, knl2); // column-major
decisions.set_format_for(F, {{0,1}}, knl2); // row-major
decisions.set_format_for(F, {{1,0}}, knl3); // column-major

// Generate and run the computations with format conversions
auto computation = decisions.generate();
computation();
\end{lstlisting}
\end{figure}

Finally, in Chapter~\ref{chap:SparseRAJA}, I develop prototype support for representing format-agnostic sparse computation and data within RAJA\@.
The two challenges for sparse computations are constructing the sparse iteration space and efficiently iterating through the sparse data.
Using symbolic evaluation to gather the data access patterns in the computation, the prototype constructs a sparse iteration space from the dense-like specification provided by the programmer.
Then, for iterating through sparse data, the system uses a software prefetching approach during kernel execution to reduce the computational complexity of data access.
While the system is outperformed by a format-specialized implementation, I identify a number of opportunities for further optimization and argue that in terms of computational complexity, the sparse prototype can eventually achieve comparable performance.
These optimizations mainly focus on the structures used to represent and traverse the iteration space, which rely on multiple levels of virtualization to meet iterator type requirements imposed by the RAJA execution model.
\begin{figure}
\begin{lstlisting}[caption={Implementation of SpMV using the SparseRAJA prototype},label=Intro:SparseRAJAMV]
DenseView<1> x(Nj);
DenseView<1> y(Ni);
SparseView<2> A(Ni,Nj);

using POLICY = KernelPolicy<
  statement::For<0,loop_exec,
    statement::For<1,loop_exec,
      statement::Lambda<0>
    >
  >
>;

auto seg1 = RangeSegment(0,Ni);
auto seg2 = RangeSegment(0,Nj);
auto dense_segs = make_tuple(seg1, seg2);

auto lam = [&](auto i, auto j) {
  y(i) += A(i,j) * x(j);
}

auto knl = make_sparse_kernel<POLICY>(dense_segs, A, lam);
  
knl();
\end{lstlisting}
\end{figure}







